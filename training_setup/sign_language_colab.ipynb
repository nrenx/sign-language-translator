{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Installation Strategy for Google Colab (Python 3.12, Nov 2025)\n",
    "# - Google Colab now uses Python 3.12 with pre-installed TensorFlow 2.19+ ecosystem\n",
    "# - Must work with Colab's native package versions to avoid binary incompatibilities\n",
    "# - Strategy: Clean conflicting packages, then install only what's needed\n",
    "# - Let pip resolve compatible versions automatically (no forced pinning)\n",
    "\n",
    "# Step 1: Remove conflicting pre-installed packages\n",
    "print('Removing conflicting packages...')\n",
    "!pip uninstall -y tensorflow-decision-forests tensorflowjs tensorflow-text tf-keras ydf -qq\n",
    "\n",
    "# Step 2: Reinstall TensorFlow ecosystem with compatible versions\n",
    "print('\\nReinstalling TensorFlow stack (this may take 2-3 minutes)...')\n",
    "!pip install -q --upgrade --force-reinstall \\\n",
    "    \"tensorflow>=2.17.0\" \\\n",
    "    \"tensorflowjs>=4.20.0\" \\\n",
    "    \"mediapipe>=0.10.14\" \\\n",
    "    \"protobuf>=4.25.0,<5.0.0\"\n",
    "\n",
    "# Step 3: Install additional ML libraries\n",
    "print('\\nInstalling additional libraries...')\n",
    "!pip install -q \\\n",
    "    \"kagglehub\" \\\n",
    "    \"scikit-learn\" \\\n",
    "    \"matplotlib\" \\\n",
    "    \"seaborn\" \\\n",
    "    \"tqdm\" \\\n",
    "    \"opencv-python-headless\"\n",
    "\n",
    "print('\\nâœ“ Installation complete!\\n')\n",
    "\n",
    "# Verify critical package versions\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import google.protobuf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"\\nâœ“ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ“ MediaPipe version: {mp.__version__}\")\n",
    "print(f\"âœ“ Protobuf version: {google.protobuf.__version__}\")\n",
    "print(f\"âœ“ NumPy version: {np.__version__}\")\n",
    "print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nâœ“ GPU Available: {len(gpus)} GPU(s) detected\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "else:\n",
    "    print(\"\\nâš  No GPU detected - training will use CPU (slower)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Mount Drive and configure paths/seeds\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# - Mount Google Drive with error handling\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print('âœ“ Google Drive mounted successfully')\n",
    "except Exception as e:\n",
    "    print(f'âš  Warning: Could not mount Drive: {e}')\n",
    "    print('  Continuing without Drive - artifacts will not persist after session ends')\n",
    "\n",
    "# - Fix seeds for fully reproducible splits and training\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "# Enable deterministic operations for full reproducibility\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "print(f'âœ“ Random seeds set to {SEED} for reproducibility')\n",
    "\n",
    "# - Declare canonical paths used throughout the pipeline\n",
    "DATASET_KAGGLE = 'kapillondhe/american-sign-language'\n",
    "COLAB_EXTRACT_DIR = '/kaggle/input/american-sign-language/ASL_Dataset'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/sign_language_training'\n",
    "RAW_LANDMARKS_NPZ = os.path.join(OUTPUT_DIR, 'hand_landmarks_raw.npz')\n",
    "PROC_NPZ = os.path.join(OUTPUT_DIR, 'hand_landmarks_processed.npz')\n",
    "BEST_H5 = os.path.join(OUTPUT_DIR, 'best_model.h5')\n",
    "BEST_KERAS = os.path.join(OUTPUT_DIR, 'best_model.keras')\n",
    "SAVED_MODEL_DIR = os.path.join(OUTPUT_DIR, 'saved_model')\n",
    "TFJS_DIR = os.path.join(OUTPUT_DIR, 'tfjs_model')\n",
    "LABELS_JSON = os.path.join(OUTPUT_DIR, 'labels.json')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'\\nâœ“ Output directory: {OUTPUT_DIR}')\n",
    "print(f'âœ“ Dataset will be downloaded to: {COLAB_EXTRACT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e955462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download ASL dataset via kagglehub and validate structure\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "print('Downloading dataset from Kaggle...')\n",
    "print(f'Dataset: {DATASET_KAGGLE}')\n",
    "\n",
    "try:\n",
    "    # - Pull the dataset via kagglehub (handles authentication automatically in Colab)\n",
    "    path = kagglehub.dataset_download(DATASET_KAGGLE)\n",
    "    print(f'âœ“ kagglehub download complete: {path}')\n",
    "except Exception as e:\n",
    "    print(f'âŒ Error downloading dataset: {e}')\n",
    "    print('âš  Make sure you have internet connection in Colab')\n",
    "    raise\n",
    "\n",
    "# Use the actual kagglehub download path, not the hardcoded COLAB_EXTRACT_DIR\n",
    "dataset_root = Path(path)\n",
    "\n",
    "# Check if ASL_Dataset subdirectory exists\n",
    "if (dataset_root / 'ASL_Dataset').exists():\n",
    "    dataset_root = dataset_root / 'ASL_Dataset'\n",
    "    print(f'âœ“ Found ASL_Dataset subdirectory')\n",
    "\n",
    "if not dataset_root.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'Dataset path does not exist: {dataset_root}\\n'\n",
    "        f'Please check the dataset structure.'\n",
    "    )\n",
    "\n",
    "# Validate class folders\n",
    "class_dirs = sorted([p for p in dataset_root.iterdir() if p.is_dir()])\n",
    "if not class_dirs:\n",
    "    raise ValueError(f'No class directories found in {dataset_root}')\n",
    "\n",
    "class_names = [p.name for p in class_dirs]\n",
    "\n",
    "# Expected ASL letters (A-Y excluding J and Z which require motion)\n",
    "expected_letters = [chr(code) for code in range(ord('A'), ord('Z') + 1) \n",
    "                   if chr(code) not in {'J', 'Z'}]\n",
    "missing = [letter for letter in expected_letters if letter not in class_names]\n",
    "\n",
    "print(f'\\nâœ“ Using dataset root: {dataset_root}')\n",
    "print(f'âœ“ Found {len(class_dirs)} class folders: {class_names}')\n",
    "\n",
    "if missing:\n",
    "    print(f'âš  WARNING: Missing expected classes: {missing}')\n",
    "    print('  Training will proceed with available classes only')\n",
    "else:\n",
    "    print('âœ“ All expected ASL classes (A-Y excluding J/Z) are present')\n",
    "\n",
    "# Count sample images per class\n",
    "total_images = 0\n",
    "for class_dir in class_dirs:\n",
    "    image_count = len(list(class_dir.glob('**/*.*')))\n",
    "    total_images += image_count\n",
    "print(f'\\nâœ“ Total images across all classes: {total_images:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract wrist-centered MediaPipe landmarks from images\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print('Initializing MediaPipe Hands...')\n",
    "\n",
    "# - Run MediaPipe Hands on every class folder and save wrist-centered landmark tensors\n",
    "hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "labels = [d.name for d in class_dirs]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "all_vectors, all_labels = [], []\n",
    "class_counts = {label: 0 for label in labels}\n",
    "skipped = 0\n",
    "total_images = 0\n",
    "\n",
    "print(f'Processing {len(class_dirs)} classes...\\n')\n",
    "\n",
    "for label_dir in class_dirs:\n",
    "    # Get all image files (common formats)\n",
    "    image_files = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "        image_files.extend(list(label_dir.glob(f'**/{ext}')))\n",
    "    image_files = sorted(set(image_files))  # Remove duplicates\n",
    "\n",
    "    if not image_files:\n",
    "        print(f'âš  No images found in {label_dir.name}')\n",
    "        continue\n",
    "\n",
    "    total_images += len(image_files)\n",
    "\n",
    "    for img_path in tqdm(image_files, desc=f'Extracting {label_dir.name}', leave=False):\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Convert BGR to RGB for MediaPipe\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(rgb)\n",
    "\n",
    "            if not result.multi_hand_landmarks:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Extract landmarks and apply wrist-centering normalization\n",
    "            lm_array = np.array(\n",
    "                [[lm.x, lm.y, lm.z] for lm in result.multi_hand_landmarks[0].landmark],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            wrist_centered = lm_array - lm_array[0]  # Subtract wrist position\n",
    "            vector = wrist_centered.flatten()  # Shape: (63,)\n",
    "\n",
    "            all_vectors.append(vector)\n",
    "            all_labels.append(label_to_idx[label_dir.name])\n",
    "            class_counts[label_dir.name] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'âš  Error processing {img_path.name}: {e}')\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "    # Free memory periodically\n",
    "    gc.collect()\n",
    "\n",
    "hands.close()\n",
    "print('\\nâœ“ MediaPipe processing complete')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.stack(all_vectors, dtype=np.float32)\n",
    "y = np.array(all_labels, dtype=np.int32)\n",
    "\n",
    "# Save raw landmarks\n",
    "np.savez(\n",
    "    RAW_LANDMARKS_NPZ,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    labels=np.array(labels),\n",
    "    class_counts=np.array([class_counts[label] for label in labels]),\n",
    "    skipped=np.array([skipped]),\n",
    "    total_images=np.array([total_images])\n",
    ")\n",
    "\n",
    "skip_ratio = skipped / max(total_images, 1)\n",
    "print(f'\\nðŸ“Š Extraction Summary:')\n",
    "print(f'   Total images processed: {total_images:,}')\n",
    "print(f'   Successfully extracted: {len(X):,} samples')\n",
    "print(f'   Skipped (no hand detected): {skipped:,} ({skip_ratio:.1%})')\n",
    "\n",
    "if skip_ratio > 0.2:\n",
    "    print('âš  WARNING: Over 20% images skipped.')\n",
    "    print('  Consider: (1) Lowering detection threshold, or (2) Reviewing data quality')\n",
    "\n",
    "print(f'\\nâœ“ Saved raw landmarks to: {RAW_LANDMARKS_NPZ}')\n",
    "print(f'\\nPer-class sample counts:')\n",
    "for label in sorted(class_counts.keys()):\n",
    "    print(f'   {label}: {class_counts[label]:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce906b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Preprocess, augment, and create stratified train/val/test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading raw landmarks...')\n",
    "raw = np.load(RAW_LANDMARKS_NPZ, allow_pickle=True)\n",
    "X = raw['X']\n",
    "y = raw['y']\n",
    "labels = raw['labels']\n",
    "\n",
    "print(f'âœ“ Loaded {len(X):,} samples with {len(labels)} classes')\n",
    "\n",
    "# Stratified deterministic splits: 75% train, 15% val, 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=SEED, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.4, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f'âœ“ Initial splits: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}')\n",
    "\n",
    "# Data augmentation (optional but recommended)\n",
    "ENABLE_AUGMENTATION = True  # Toggle to quickly disable augmentations\n",
    "\n",
    "if ENABLE_AUGMENTATION:\n",
    "    print('\\nApplying data augmentation...')\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    augmented_X, augmented_y = [], []\n",
    "    \n",
    "    for vec, label in tqdm(zip(X_train, y_train), total=len(X_train), desc='Augmenting'):\n",
    "        # Augmentation 1: Add Gaussian noise (Ïƒ=0.01)\n",
    "        noise = vec + rng.normal(0, 0.01, size=vec.shape)\n",
    "        augmented_X.append(noise.astype(np.float32))\n",
    "        augmented_y.append(label)\n",
    "        \n",
    "        # Augmentation 2: Horizontal flip (mirror x-coordinates)\n",
    "        mirrored = vec.copy()\n",
    "        mirrored[0::3] *= -1  # Flip every 3rd element (x coordinates)\n",
    "        augmented_X.append(mirrored.astype(np.float32))\n",
    "        augmented_y.append(label)\n",
    "    \n",
    "    # Combine original + augmented data\n",
    "    X_train = np.concatenate([X_train, np.stack(augmented_X)], axis=0)\n",
    "    y_train = np.concatenate([y_train, np.array(augmented_y)])\n",
    "    \n",
    "    print(f'âœ“ Augmentation complete: Train set expanded to {len(X_train):,} samples')\n",
    "else:\n",
    "    print('âš  Augmentation disabled')\n",
    "\n",
    "# Save processed splits\n",
    "np.savez(\n",
    "    PROC_NPZ,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_val=X_val, y_val=y_val,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "print(f'\\nâœ“ Processed splits saved to: {PROC_NPZ}')\n",
    "print(f'\\nðŸ“Š Final split sizes:')\n",
    "print(f'   Train: {X_train.shape}')\n",
    "print(f'   Val:   {X_val.shape}')\n",
    "print(f'   Test:  {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Build and train MLP classifier with callbacks and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "print('Loading processed data...')\n",
    "proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "X_train, y_train = proc['X_train'], proc['y_train']\n",
    "X_val, y_val = proc['X_val'], proc['y_val']\n",
    "num_classes = len(proc['labels'])\n",
    "\n",
    "print(f'âœ“ Train: {X_train.shape}, Val: {X_val.shape}')\n",
    "print(f'âœ“ Number of classes: {num_classes}')\n",
    "\n",
    "# Build MLP architecture\n",
    "print('\\nBuilding model...')\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(63,), name='landmark_input'),\n",
    "    layers.Dense(256, activation='relu', name='dense1'),\n",
    "    layers.BatchNormalization(name='bn1'),\n",
    "    layers.Dropout(0.3, name='dropout1'),\n",
    "    layers.Dense(128, activation='relu', name='dense2'),\n",
    "    layers.Dropout(0.2, name='dropout2'),\n",
    "    layers.Dense(64, activation='relu', name='dense3'),\n",
    "    layers.Dense(num_classes, activation='softmax', name='output')\n",
    "], name='ASL_Classifier')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('âœ“ Model compiled')\n",
    "model.summary()\n",
    "\n",
    "# Configure callbacks\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    BEST_KERAS,  # Save in modern Keras format\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "earlystop_cb = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_cb = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\nStarting training...')\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print('\\nâœ“ Training complete!')\n",
    "\n",
    "# Visualize training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.title('Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nâœ“ Best model saved to: {BEST_KERAS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate best model on test set and log detailed metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print('Loading test data and best model...')\n",
    "proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "X_test, y_test, labels = proc['X_test'], proc['y_test'], proc['labels']\n",
    "\n",
    "# Load best model from training\n",
    "best_model = tf.keras.models.load_model(BEST_KERAS)\n",
    "print('âœ“ Model loaded')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nðŸ“Š Test Set Performance:')\n",
    "print(f'   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'   Loss: {test_loss:.4f}')\n",
    "\n",
    "# Generate predictions\n",
    "y_probs = best_model.predict(X_test, verbose=0)\n",
    "y_pred = y_probs.argmax(axis=1)\n",
    "\n",
    "# Classification report\n",
    "print('\\n' + '='*70)\n",
    "print('CLASSIFICATION REPORT')\n",
    "print('='*70)\n",
    "print(classification_report(y_test, y_pred, target_names=labels, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save sample predictions\n",
    "sample_df = pd.DataFrame({\n",
    "    'true_label': [labels[idx] for idx in y_test[:50]],\n",
    "    'pred_label': [labels[idx] for idx in y_pred[:50]],\n",
    "    'confidence': y_probs[:50].max(axis=1),\n",
    "    'correct': [y_test[i] == y_pred[i] for i in range(min(50, len(y_test)))]\n",
    "})\n",
    "\n",
    "sample_csv_path = os.path.join(OUTPUT_DIR, 'sample_predictions.csv')\n",
    "sample_df.to_csv(sample_csv_path, index=False)\n",
    "print(f'\\nâœ“ Sample predictions saved to: {sample_csv_path}')\n",
    "print(f'\\nFirst 10 predictions:')\n",
    "print(sample_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Export model in multiple formats (Keras, SavedModel, labels)\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Preparing to export model artifacts...')\n",
    "\n",
    "# Clean up previous exports to avoid conflicts\n",
    "if os.path.exists(SAVED_MODEL_DIR):\n",
    "    shutil.rmtree(SAVED_MODEL_DIR)\n",
    "    print('âœ“ Cleaned old SavedModel directory')\n",
    "\n",
    "if os.path.exists(TFJS_DIR):\n",
    "    shutil.rmtree(TFJS_DIR)\n",
    "    print('âœ“ Cleaned old TFJS directory')\n",
    "\n",
    "os.makedirs(TFJS_DIR, exist_ok=True)\n",
    "\n",
    "# Load the best model\n",
    "print(f'\\nLoading best model from: {BEST_KERAS}')\n",
    "model = tf.keras.models.load_model(BEST_KERAS)\n",
    "print('âœ“ Model loaded successfully')\n",
    "\n",
    "# Export 1: SavedModel format (standard TensorFlow format)\n",
    "print(f'\\nExporting to SavedModel format...')\n",
    "model.export(SAVED_MODEL_DIR)  # TF 2.16+ uses export() instead of save()\n",
    "print(f'âœ“ SavedModel saved: {SAVED_MODEL_DIR}')\n",
    "\n",
    "# Verify SavedModel structure\n",
    "if os.path.exists(os.path.join(SAVED_MODEL_DIR, 'saved_model.pb')):\n",
    "    print('  âœ“ saved_model.pb exists')\n",
    "if os.path.exists(os.path.join(SAVED_MODEL_DIR, 'variables')):\n",
    "    print('  âœ“ variables/ directory exists')\n",
    "\n",
    "# Export 2: Create labels mapping JSON\n",
    "print(f'\\nCreating labels mapping...')\n",
    "proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "labels = proc['labels']\n",
    "label_map = {int(idx): str(label) for idx, label in enumerate(labels)}\n",
    "\n",
    "with open(LABELS_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_map, f, indent=2, ensure_ascii=False)\n",
    "print(f'âœ“ Labels saved: {LABELS_JSON}')\n",
    "print(f'  Classes: {list(label_map.values())}')\n",
    "\n",
    "# Summary of exported artifacts\n",
    "print('\\n' + '='*70)\n",
    "print('EXPORT SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "def get_size_mb(path):\n",
    "    \"\"\"Calculate directory or file size in MB\"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        return os.path.getsize(path) / (1024 * 1024)\n",
    "    total = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            total += os.path.getsize(os.path.join(root, f))\n",
    "    return total / (1024 * 1024)\n",
    "\n",
    "print(f'\\nâœ“ Keras Model (.keras):     {get_size_mb(BEST_KERAS):.2f} MB')\n",
    "print(f'âœ“ SavedModel (directory):   {get_size_mb(SAVED_MODEL_DIR):.2f} MB')\n",
    "print(f'âœ“ Labels JSON:              {get_size_mb(LABELS_JSON):.3f} MB')\n",
    "\n",
    "print('\\nðŸŽ‰ All model artifacts exported successfully!')\n",
    "print(f'\\nNext step: Run the TFJS conversion cell to create browser-compatible model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Convert SavedModel to TensorFlow.js format for browser deployment\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print('Converting model to TensorFlow.js format...')\n",
    "print(f'Source: {SAVED_MODEL_DIR}')\n",
    "print(f'Target: {TFJS_DIR}\\n')\n",
    "\n",
    "try:\n",
    "    # Check if tensorflowjs_converter is available\n",
    "    check_result = subprocess.run(\n",
    "        ['which', 'tensorflowjs_converter'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if check_result.returncode != 0:\n",
    "        print('âš  tensorflowjs_converter not found in PATH, trying pip installation...')\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'tensorflowjs'], check=True)\n",
    "        print('âœ“ tensorflowjs installed')\n",
    "    \n",
    "    # Run conversion\n",
    "    print('Running conversion (this may take 1-2 minutes)...')\n",
    "    result = subprocess.run([\n",
    "        'tensorflowjs_converter',\n",
    "        '--input_format=tf_saved_model',\n",
    "        '--output_format=tfjs_graph_model',\n",
    "        '--signature_name=serving_default',\n",
    "        '--strip_debug_ops=True',\n",
    "        SAVED_MODEL_DIR,\n",
    "        TFJS_DIR\n",
    "    ], check=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    \n",
    "    print('\\nâœ“ TFJS conversion complete!')\n",
    "    \n",
    "    # List generated files\n",
    "    print(f'\\nðŸ“¦ Generated TFJS artifacts:')\n",
    "    tfjs_files = sorted(os.listdir(TFJS_DIR))\n",
    "    total_size = 0\n",
    "    \n",
    "    for item in tfjs_files:\n",
    "        item_path = os.path.join(TFJS_DIR, item)\n",
    "        size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f'   â€¢ {item:<30} {size_mb:>8.2f} MB')\n",
    "    \n",
    "    print(f'\\n   Total TFJS model size: {total_size:.2f} MB')\n",
    "    \n",
    "    # Verify critical files\n",
    "    model_json = os.path.join(TFJS_DIR, 'model.json')\n",
    "    if os.path.exists(model_json):\n",
    "        print(f'\\nâœ“ model.json exists - ready for web deployment')\n",
    "        print(f'  Load in browser with: tf.loadGraphModel(\"path/to/model.json\")')\n",
    "    else:\n",
    "        print('\\nâš  WARNING: model.json not found!')\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f'âŒ ERROR: Command not found - {e}')\n",
    "    print('Make sure tensorflowjs is installed:')\n",
    "    print('  !pip install tensorflowjs')\n",
    "    sys.exit(1)\n",
    "    \n",
    "except subprocess.CalledProcessError as exc:\n",
    "    print(f'âŒ TFJS conversion failed with exit code {exc.returncode}')\n",
    "    print(f'\\nSTDOUT:\\n{exc.stdout}')\n",
    "    print(f'\\nSTDERR:\\n{exc.stderr}')\n",
    "    print('\\nTroubleshooting:')\n",
    "    print('1. Check that SavedModel format is valid')\n",
    "    print('2. Try updating tensorflowjs: !pip install -U tensorflowjs')\n",
    "    print('3. Verify sufficient disk space in Drive')\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'âŒ Unexpected error: {e}')\n",
    "    raise\n",
    "\n",
    "print('\\nðŸŽ‰ Model is ready for browser deployment!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c502e",
   "metadata": {},
   "source": [
    "# ðŸŒ Runtime Integration Guide\n",
    "\n",
    "This cell provides reference code for deploying the trained model in a web browser.\n",
    "\n",
    "## Browser Deployment Steps\n",
    "\n",
    "1. **Install TensorFlow.js and Hand Detection**\n",
    "```bash\n",
    "npm install @tensorflow/tfjs\n",
    "npm install @tensorflow-models/hand-pose-detection\n",
    "```\n",
    "\n",
    "2. **Load the TFJS model and labels**\n",
    "```javascript\n",
    "// Load the classifier model\n",
    "const model = await tf.loadGraphModel('/path/to/tfjs_model/model.json');\n",
    "\n",
    "// Load labels\n",
    "const labelsResponse = await fetch('/path/to/labels.json');\n",
    "const labels = await labelsResponse.json();\n",
    "```\n",
    "\n",
    "3. **Initialize MediaPipe Hands detector**\n",
    "```javascript\n",
    "const detector = await handPoseDetection.createDetector(\n",
    "  handPoseDetection.SupportedModels.MediaPipeHands,\n",
    "  {\n",
    "    runtime: 'tfjs',\n",
    "    maxHands: 1,\n",
    "    modelType: 'full'\n",
    "  }\n",
    ");\n",
    "```\n",
    "\n",
    "4. **Process video frames and classify**\n",
    "```javascript\n",
    "async function classifyFrame(videoElement) {\n",
    "  // Detect hand landmarks\n",
    "  const hands = await detector.estimateHands(videoElement, {\n",
    "    flipHorizontal: true\n",
    "  });\n",
    "  \n",
    "  if (!hands || hands.length === 0) {\n",
    "    return null; // No hand detected\n",
    "  }\n",
    "  \n",
    "  // Get 3D keypoints (21 landmarks)\n",
    "  const keypoints = hands[0].keypoints3D;\n",
    "  \n",
    "  // Apply wrist-centering normalization\n",
    "  const wrist = keypoints[0];\n",
    "  const wristCentered = keypoints.flatMap(pt => [\n",
    "    pt.x - wrist.x,\n",
    "    pt.y - wrist.y,\n",
    "    pt.z - wrist.z\n",
    "  ]);\n",
    "  \n",
    "  // Create input tensor [1, 63]\n",
    "  const inputTensor = tf.tensor2d([wristCentered], [1, 63]);\n",
    "  \n",
    "  // Run inference\n",
    "  const predictions = model.predict(inputTensor);\n",
    "  const probabilities = await predictions.data();\n",
    "  \n",
    "  // Get top prediction\n",
    "  const maxIdx = probabilities.indexOf(Math.max(...probabilities));\n",
    "  const confidence = probabilities[maxIdx];\n",
    "  \n",
    "  // Clean up tensors\n",
    "  inputTensor.dispose();\n",
    "  predictions.dispose();\n",
    "  \n",
    "  return {\n",
    "    letter: labels[maxIdx],\n",
    "    confidence: confidence\n",
    "  };\n",
    "}\n",
    "```\n",
    "\n",
    "5. **Implement smoothing (optional but recommended)**\n",
    "```javascript\n",
    "// Use majority voting over N frames to reduce jitter\n",
    "class PredictionSmoother {\n",
    "  constructor(windowSize = 5) {\n",
    "    this.window = [];\n",
    "    this.windowSize = windowSize;\n",
    "  }\n",
    "  \n",
    "  addPrediction(letter, confidence) {\n",
    "    if (confidence > 0.7) { // Confidence threshold\n",
    "      this.window.push(letter);\n",
    "      if (this.window.length > this.windowSize) {\n",
    "        this.window.shift();\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  getStablePrediction() {\n",
    "    if (this.window.length < 3) return null;\n",
    "    \n",
    "    // Count occurrences\n",
    "    const counts = {};\n",
    "    this.window.forEach(letter => {\n",
    "      counts[letter] = (counts[letter] || 0) + 1;\n",
    "    });\n",
    "    \n",
    "    // Return most frequent\n",
    "    return Object.keys(counts).reduce((a, b) => \n",
    "      counts[a] > counts[b] ? a : b\n",
    "    );\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- **Wrist-centering is critical**: Subtract landmark[0] from all landmarks before inference\n",
    "- **Input shape**: Model expects `[1, 63]` (21 landmarks Ã— 3 coordinates, flattened)\n",
    "- **Coordinate order**: Each landmark is `[x, y, z]`\n",
    "- **CORS**: Host TFJS files on same domain as your web app to avoid CORS issues\n",
    "- **Performance**: Use `requestAnimationFrame` for smooth real-time detection\n",
    "\n",
    "## Hosting\n",
    "\n",
    "Copy the `tfjs_model/` directory and `labels.json` to your web server's static assets folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa708296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Generate README and list all exported artifacts\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('Generating artifact manifest...\\n')\n",
    "\n",
    "# Create comprehensive README\n",
    "readme_path = os.path.join(OUTPUT_DIR, 'README.txt')\n",
    "readme_content = f\"\"\"\n",
    "================================================================================\n",
    "ASL SIGN LANGUAGE LANDMARK CLASSIFIER - TRAINING ARTIFACTS\n",
    "================================================================================\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Training Framework: TensorFlow {tf.__version__}\n",
    "Python Version: {sys.version.split()[0]}\n",
    "\n",
    "================================================================================\n",
    "DATASET INFORMATION\n",
    "================================================================================\n",
    "\n",
    "Dataset: {DATASET_KAGGLE}\n",
    "Source: Kaggle (via kagglehub)\n",
    "Expected Classes: 24 ASL letters (A-Y, excluding J and Z which require motion)\n",
    "\n",
    "================================================================================\n",
    "MODEL ARCHITECTURE\n",
    "================================================================================\n",
    "\n",
    "Type: Sequential MLP (Multi-Layer Perceptron)\n",
    "Input: 63 features (21 MediaPipe hand landmarks Ã— 3 coordinates, wrist-centered)\n",
    "\n",
    "Layers:\n",
    "  1. Dense(256, relu) + BatchNormalization + Dropout(0.3)\n",
    "  2. Dense(128, relu) + Dropout(0.2)\n",
    "  3. Dense(64, relu)\n",
    "  4. Dense(num_classes, softmax)\n",
    "\n",
    "Optimizer: Adam (lr=1e-3)\n",
    "Loss: Sparse Categorical Crossentropy\n",
    "\n",
    "================================================================================\n",
    "TRAINING CONFIGURATION\n",
    "================================================================================\n",
    "\n",
    "Random Seed: {SEED}\n",
    "Train/Val/Test Split: 75% / 15% / 10%\n",
    "Data Augmentation: Gaussian noise + Horizontal flip\n",
    "Batch Size: 32\n",
    "Max Epochs: 100\n",
    "Early Stopping: Patience=7 (monitor val_accuracy)\n",
    "\n",
    "================================================================================\n",
    "EXPORTED ARTIFACTS\n",
    "================================================================================\n",
    "\n",
    "1. best_model.keras\n",
    "   - Modern Keras format (TF 2.16+)\n",
    "   - Best model based on validation accuracy\n",
    "   - Use: tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "2. saved_model/ (directory)\n",
    "   - Standard TensorFlow SavedModel format\n",
    "   - Contains: saved_model.pb + variables/\n",
    "   - Use: tf.saved_model.load('saved_model')\n",
    "\n",
    "3. tfjs_model/ (directory)\n",
    "   - TensorFlow.js graph model format\n",
    "   - For browser-based inference\n",
    "   - Load: tf.loadGraphModel('tfjs_model/model.json')\n",
    "\n",
    "4. labels.json\n",
    "   - Class index to letter mapping\n",
    "   - Format: {{\"0\": \"A\", \"1\": \"B\", ...}}\n",
    "\n",
    "5. hand_landmarks_raw.npz\n",
    "   - Raw extracted landmarks from all images\n",
    "   - Arrays: X, y, labels, class_counts, skipped, total_images\n",
    "\n",
    "6. hand_landmarks_processed.npz\n",
    "   - Preprocessed and augmented training data\n",
    "   - Arrays: X_train, y_train, X_val, y_val, X_test, y_test, labels\n",
    "\n",
    "7. sample_predictions.csv\n",
    "   - First 50 test set predictions\n",
    "   - Columns: true_label, pred_label, confidence, correct\n",
    "\n",
    "================================================================================\n",
    "USAGE NOTES\n",
    "================================================================================\n",
    "\n",
    "Preprocessing Pipeline:\n",
    "  1. Extract 21 hand landmarks using MediaPipe Hands (static_image_mode=True)\n",
    "  2. Apply wrist-centering: subtract landmark[0] from all landmarks\n",
    "  3. Flatten to [63] vector\n",
    "  4. Feed to classifier\n",
    "\n",
    "For Real-Time Inference:\n",
    "  - Use same preprocessing: detect hands â†’ wrist-center â†’ flatten â†’ predict\n",
    "  - Apply confidence threshold (recommended: 0.7)\n",
    "  - Use temporal smoothing (e.g., 5-frame majority voting) to reduce jitter\n",
    "\n",
    "Browser Deployment:\n",
    "  - Use @tensorflow-models/hand-pose-detection for landmark extraction\n",
    "  - Load tfjs_model/model.json with tf.loadGraphModel()\n",
    "  - Ensure wrist-centering preprocessing matches training\n",
    "\n",
    "================================================================================\n",
    "FILE PATHS\n",
    "================================================================================\n",
    "\n",
    "Output Directory: {OUTPUT_DIR}\n",
    "\n",
    "Keras Model:      {BEST_KERAS}\n",
    "SavedModel:       {SAVED_MODEL_DIR}\n",
    "TFJS Model:       {TFJS_DIR}\n",
    "Labels:           {LABELS_JSON}\n",
    "Raw Landmarks:    {RAW_LANDMARKS_NPZ}\n",
    "Processed Data:   {PROC_NPZ}\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f'âœ“ README saved to: {readme_path}\\n')\n",
    "\n",
    "# List all artifacts with sizes\n",
    "print('='*70)\n",
    "print('ARTIFACT INVENTORY')\n",
    "print('='*70)\n",
    "\n",
    "def list_output_files(base_dir):\n",
    "    \"\"\"Recursively list all files with sizes\"\"\"\n",
    "    items = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in sorted(files):\n",
    "            path = os.path.join(root, file)\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            rel_path = os.path.relpath(path, base_dir)\n",
    "            items.append((rel_path, size_mb))\n",
    "    return sorted(items)\n",
    "\n",
    "total_size = 0\n",
    "for rel_path, size_mb in list_output_files(OUTPUT_DIR):\n",
    "    total_size += size_mb\n",
    "    print(f'{rel_path:<50} {size_mb:>10.2f} MB')\n",
    "\n",
    "print('='*70)\n",
    "print(f'{'TOTAL SIZE':<50} {total_size:>10.2f} MB')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\nâœ… All artifacts successfully saved to Google Drive!')\n",
    "print(f'\\nðŸ“‚ Access your files at: {OUTPUT_DIR}')\n",
    "print(f'\\nðŸ’¡ Tip: Download the entire folder to use in your web application')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
