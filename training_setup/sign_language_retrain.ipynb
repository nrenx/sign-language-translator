{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227630a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1A: CHECK GPU AVAILABILITY\n",
    "import subprocess\n",
    "\n",
    "print('Checking GPU availability...\\n')\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "    print('‚úì GPU detected! Training will be accelerated.')\n",
    "    print('GPU Info:')\n",
    "    print(gpu_info)\n",
    "except:\n",
    "    print('‚ö† No GPU detected. Training will use CPU (slower).')\n",
    "    print('\\nüí° TIP: Enable GPU for 10x faster training:')\n",
    "    print('   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí T4 GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaba632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1B: INSTALL REQUIRED PACKAGES\n",
    "print('Checking package installation...\\n')\n",
    "\n",
    "# Always install in Colab to ensure correct versions\n",
    "# This avoids import errors from incompatible versions\n",
    "import sys\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('üîß Installing compatible packages for Colab...')\n",
    "    print('   Installing MediaPipe, NumPy, and dependencies\\n')\n",
    "    \n",
    "    # Install compatible versions\n",
    "    # - mediapipe 0.10.13: Earliest available version in current PyPI\n",
    "    # - numpy 1.26.4: Compatible with TF 2.19 and MediaPipe\n",
    "    # - opencv-python: Ensure compatible version\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q 'mediapipe==0.10.13' 'numpy==1.26.4' 'opencv-python'\n",
    "    !pip install -q tensorflowjs seaborn tqdm\n",
    "    \n",
    "    print('='*60)\n",
    "    print('‚úì Installation complete!')\n",
    "    print('='*60)\n",
    "    print('\\nüì¶ Installed packages:')\n",
    "    print('   ‚Ä¢ MediaPipe: 0.10.13')\n",
    "    print('   ‚Ä¢ NumPy: 1.26.4')\n",
    "    print('   ‚Ä¢ OpenCV: latest compatible')\n",
    "    print('\\nüìã NEXT STEPS:')\n",
    "    print('   Continue running the remaining cells')\n",
    "    print('='*60)\n",
    "else:\n",
    "    # Local environment - check if packages need installation\n",
    "    needs_install = False\n",
    "    install_reason = \"\"\n",
    "    \n",
    "    try:\n",
    "        import mediapipe as mp\n",
    "        import numpy as np\n",
    "        \n",
    "        # Check versions\n",
    "        numpy_version = tuple(map(int, np.__version__.split('.')[:2]))\n",
    "        \n",
    "        # Verify compatible versions are installed\n",
    "        if numpy_version >= (2, 0):\n",
    "            needs_install = True\n",
    "            install_reason = \"NumPy 2.x detected (incompatible)\"\n",
    "        else:\n",
    "            print('‚úì Packages already installed')\n",
    "            print(f'  ‚Ä¢ MediaPipe: {mp.__version__}')\n",
    "            print(f'  ‚Ä¢ NumPy: {np.__version__}')\n",
    "            print('  Skipping installation...\\n')\n",
    "            \n",
    "    except ImportError as e:\n",
    "        needs_install = True\n",
    "        install_reason = \"Packages not found\"\n",
    "        print(f'‚ö† {install_reason}\\n')\n",
    "    \n",
    "    if needs_install:\n",
    "        if install_reason:\n",
    "            print(f'‚ö† Installation required: {install_reason}\\n')\n",
    "        \n",
    "        print('Installing required packages...')\n",
    "        !pip install -q mediapipe 'numpy==1.26.4' opencv-python\n",
    "        !pip install -q tensorflowjs seaborn tqdm\n",
    "        \n",
    "        print('‚úì Installation complete!\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1C: IMPORT ALL LIBRARIES\n",
    "try:\n",
    "    import os\n",
    "    import json\n",
    "    import sys\n",
    "    import shutil\n",
    "    import datetime\n",
    "    import subprocess\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    \n",
    "    # Import MediaPipe with workaround for audio_classifier bug\n",
    "    try:\n",
    "        import mediapipe as mp\n",
    "    except NameError as e:\n",
    "        if 'audio_classifier' in str(e):\n",
    "            # Workaround: Import only the vision solution we need\n",
    "            print('‚ö† MediaPipe audio module has import error (known bug)')\n",
    "            print('  Applying workaround: importing vision solutions directly...\\n')\n",
    "            \n",
    "            import mediapipe.python.solutions as solutions\n",
    "            # Create a minimal mp object with just what we need\n",
    "            class MediaPipeWrapper:\n",
    "                solutions = solutions\n",
    "            mp = MediaPipeWrapper()\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print('='*60)\n",
    "    print('‚úì All imports successful!')\n",
    "    print('='*60)\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print('\\n' + '='*60)\n",
    "    print('‚ö† Error During Import!')\n",
    "    print('='*60)\n",
    "    print(f'Error: {type(e).__name__}: {e}')\n",
    "    print('\\nFull traceback:')\n",
    "    traceback.print_exc()\n",
    "    print('\\nüìã TROUBLESHOOTING:')\n",
    "    print('1. Runtime ‚Üí Restart runtime (CRITICAL - clears old imports)')\n",
    "    print('2. Re-run Cell 1 (GPU check)')\n",
    "    print('3. Re-run Cell 2 (package installation)')\n",
    "    print('4. Then run this cell again')\n",
    "    print('='*60)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775907a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1D: CONFIGURE TENSORFLOW FOR GPU/CPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print('Configuring TensorFlow for GPU acceleration...\\n')\n",
    "    \n",
    "    # Enable mixed precision for 2-3x speedup (GPU only)\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print('‚úì Mixed precision training enabled (float16)')\n",
    "    \n",
    "    # Enable GPU memory growth (prevents OOM errors)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print('‚úì GPU memory growth enabled')\n",
    "    print('\\nüí° GPU detected - Training will be 10x faster!')\n",
    "else:\n",
    "    print('‚ö† Running on CPU - Training will take longer')\n",
    "    print('\\nüí° CPU MODE TIPS:')\n",
    "    print('  ‚Ä¢ Expected training time: 30-60 minutes (vs 3-5 min on GPU)')\n",
    "    print('  ‚Ä¢ Reduce batch size if you encounter memory errors')\n",
    "    print('  ‚Ä¢ Consider using a smaller dataset for testing')\n",
    "    print('  ‚Ä¢ GPU time limit in Colab: Enable GPU if you have quota available')\n",
    "\n",
    "print(f\"\\nüì¶ Package Versions:\")\n",
    "print(f\"  ‚Ä¢ TensorFlow: {tf.__version__}\")\n",
    "print(f\"  ‚Ä¢ MediaPipe: {mp.__version__}\")\n",
    "print(f\"  ‚Ä¢ NumPy: {np.__version__}\")\n",
    "print(f\"  ‚Ä¢ OpenCV: {cv2.__version__}\")\n",
    "\n",
    "print(f\"  ‚Ä¢ GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ SETUP COMPLETE - Ready to proceed with training!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: MOUNT GOOGLE DRIVE AND SET PATHS\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print('‚úì Google Drive mounted successfully')\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    print('‚ö† Not running in Colab - using local paths')\n",
    "    IS_COLAB = False\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "print(f'‚úì Random seeds set to {SEED}')\n",
    "\n",
    "# Configure dataset source\n",
    "# OPTION 1: Download from Kaggle (recommended for Colab)\n",
    "USE_KAGGLE_DATASET = True  # Set to False if using your own dataset\n",
    "KAGGLE_DATASET = 'kapillondhe/american-sign-language'\n",
    "\n",
    "# OPTION 2: Use custom dataset path\n",
    "CUSTOM_DATASET_PATH = \"/content/drive/MyDrive/ASL_Dataset\"  # Update if using custom dataset\n",
    "\n",
    "# Output directory (always saved to Drive in Colab)\n",
    "if IS_COLAB:\n",
    "    OUTPUT_DIR = \"/content/drive/MyDrive/asl_model_output\"\n",
    "else:\n",
    "    OUTPUT_DIR = \"asl_model_output\"\n",
    "\n",
    "print(f'\\nüìÇ Output directory: {OUTPUT_DIR}')\n",
    "print(f'\\n‚ö† DATASET SOURCE: {\"Kaggle (auto-download)\" if USE_KAGGLE_DATASET else \"Custom path\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f92edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: DOWNLOAD DATASET FROM KAGGLE (IF ENABLED)\n",
    "if USE_KAGGLE_DATASET:\n",
    "    print('Downloading dataset from Kaggle...')\n",
    "    print(f'Dataset: {KAGGLE_DATASET}')\n",
    "    \n",
    "    # Install kagglehub if not already installed\n",
    "    !pip install -q kagglehub\n",
    "    \n",
    "    import kagglehub\n",
    "    from pathlib import Path\n",
    "    \n",
    "    try:\n",
    "        # Download via kagglehub (handles authentication automatically in Colab)\n",
    "        path = kagglehub.dataset_download(KAGGLE_DATASET)\n",
    "        print(f'‚úì kagglehub download complete: {path}')\n",
    "        \n",
    "        # Use the actual download path\n",
    "        dataset_root = Path(path)\n",
    "        \n",
    "        # Check if ASL_Dataset subdirectory exists\n",
    "        if (dataset_root / 'ASL_Dataset').exists():\n",
    "            dataset_root = dataset_root / 'ASL_Dataset'\n",
    "            print(f'‚úì Found ASL_Dataset subdirectory')\n",
    "        \n",
    "        DATASET_PATH = str(dataset_root)\n",
    "        print(f'‚úì Using dataset from: {DATASET_PATH}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error downloading dataset: {e}')\n",
    "        print('‚ö† Make sure you have internet connection in Colab')\n",
    "        raise\n",
    "else:\n",
    "    # Use custom dataset path\n",
    "    DATASET_PATH = CUSTOM_DATASET_PATH\n",
    "    print(f'Using custom dataset path: {DATASET_PATH}')\n",
    "\n",
    "# Validate dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset path not found: {DATASET_PATH}\\n\"\n",
    "        f\"If using Kaggle: Check internet connection\\n\"\n",
    "        f\"If using custom: Update CUSTOM_DATASET_PATH in previous cell\"\n",
    "    )\n",
    "\n",
    "# Get class names (letter folders only)\n",
    "# Handle two dataset structures:\n",
    "# Structure 1: ASL_Dataset/A/, ASL_Dataset/B/, ... (flat)\n",
    "# Structure 2: ASL_Dataset/Train/A/, ASL_Dataset/Test/A/, ... (Train/Test split)\n",
    "folders = sorted([d for d in os.listdir(DATASET_PATH) \n",
    "                  if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
    "\n",
    "print(f\"üìÇ Dataset structure detected: {folders[:5]}...\")\n",
    "\n",
    "# Check if Train/Test folders exist\n",
    "if 'Train' in folders or 'Test' in folders:\n",
    "    print(\"‚úì Train/Test split structure detected\")\n",
    "    print(\"  Combining Train and Test folders for training...\")\n",
    "    \n",
    "    # Use Train folder for getting class names (Test should have same classes)\n",
    "    train_path = os.path.join(DATASET_PATH, 'Train')\n",
    "    test_path = os.path.join(DATASET_PATH, 'Test')\n",
    "    \n",
    "    # Get class names from Train folder\n",
    "    class_names = sorted([d for d in os.listdir(train_path) \n",
    "                          if os.path.isdir(os.path.join(train_path, d))])\n",
    "    \n",
    "    # Collect all image paths from both Train and Test\n",
    "    all_image_paths = []\n",
    "    class_image_counts = {}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_image_counts[class_name] = 0\n",
    "        \n",
    "        # Get images from Train folder\n",
    "        train_class_path = os.path.join(train_path, class_name)\n",
    "        if os.path.exists(train_class_path):\n",
    "            train_images = [os.path.join(train_class_path, f) \n",
    "                           for f in os.listdir(train_class_path) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            all_image_paths.extend([(img, class_name) for img in train_images])\n",
    "            class_image_counts[class_name] += len(train_images)\n",
    "        \n",
    "        # Get images from Test folder\n",
    "        test_class_path = os.path.join(test_path, class_name)\n",
    "        if os.path.exists(test_class_path):\n",
    "            test_images = [os.path.join(test_class_path, f) \n",
    "                          for f in os.listdir(test_class_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            all_image_paths.extend([(img, class_name) for img in test_images])\n",
    "            class_image_counts[class_name] += len(test_images)\n",
    "    \n",
    "    # Store for later use in landmark extraction\n",
    "    DATASET_STRUCTURE = 'train_test_split'\n",
    "    DATASET_IMAGE_PATHS = all_image_paths\n",
    "    total_images = len(all_image_paths)\n",
    "    \n",
    "else:\n",
    "    print(\"‚úì Flat structure detected (letter folders at root)\")\n",
    "    class_names = folders\n",
    "    \n",
    "    # Count images per class\n",
    "    class_image_counts = {}\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(DATASET_PATH, class_name)\n",
    "        count = len([f for f in os.listdir(class_path) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        class_image_counts[class_name] = count\n",
    "    \n",
    "    DATASET_STRUCTURE = 'flat'\n",
    "    total_images = sum(class_image_counts.values())\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\n‚úì Found {num_classes} classes: {class_names}\")\n",
    "\n",
    "# Check if dataset includes non-letter classes (Nothing, Space, etc.)\n",
    "non_letter_classes = [c for c in class_names if c not in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ']\n",
    "if non_letter_classes:\n",
    "    print(f\"‚ö† Non-letter classes detected: {non_letter_classes}\")\n",
    "    print(f\"  These will be included in training for better model robustness\")\n",
    "\n",
    "# Validate class count (A-Z = 26, plus optional Nothing/Space/etc)\n",
    "assert 24 <= num_classes <= 30, f\"Unexpected class count: {num_classes}\"\n",
    "print(f\"‚úì Valid class count: {num_classes}\")\n",
    "\n",
    "print(f'\\n‚úì Total images across all classes: {total_images:,}')\n",
    "print(f'\\nSample counts per class:')\n",
    "for class_name in sorted(class_image_counts.keys())[:5]:  # Show first 5\n",
    "    print(f'  {class_name}: {class_image_counts[class_name]:,}')\n",
    "if len(class_image_counts) > 5:\n",
    "    print(f'  ... and {len(class_image_counts) - 5} more classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ed5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: EXTRACT MEDIAPIPE HAND LANDMARKS\n",
    "# Initialize MediaPipe Hands with optimized settings\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    \"\"\"Extract 63 features from hand landmarks with wrist centering.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "        wrist = landmarks[0]\n",
    "        landmarks_centered = landmarks - wrist  # Center at wrist (landmark 0)\n",
    "        return landmarks_centered.flatten()\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úì MediaPipe landmark extractor ready\")\n",
    "\n",
    "# Process all images with progress tracking\n",
    "X_data = []\n",
    "y_labels = []\n",
    "skipped_count = 0\n",
    "\n",
    "print(\"\\nExtracting landmarks from images...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use tqdm for better progress tracking in Colab\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    use_tqdm = True\n",
    "except:\n",
    "    use_tqdm = False\n",
    "    print(\"Install tqdm for progress bars: !pip install tqdm\")\n",
    "\n",
    "# Handle different dataset structures\n",
    "if DATASET_STRUCTURE == 'train_test_split':\n",
    "    # Process pre-collected image paths from Train + Test folders\n",
    "    print(f\"Processing {total_images:,} images from Train and Test folders combined...\")\n",
    "    \n",
    "    if use_tqdm:\n",
    "        iterator = tqdm(DATASET_IMAGE_PATHS, desc=\"Extracting landmarks\")\n",
    "    else:\n",
    "        iterator = DATASET_IMAGE_PATHS\n",
    "    \n",
    "    for img_path, class_name in iterator:\n",
    "        landmarks = extract_landmarks(img_path)\n",
    "        \n",
    "        if landmarks is not None:\n",
    "            X_data.append(landmarks)\n",
    "            y_labels.append(class_name)\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"‚úì Processed all images from Train and Test folders\")\n",
    "    \n",
    "else:\n",
    "    # Process flat structure (original approach)\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(DATASET_PATH, class_name)\n",
    "        image_files = [f for f in os.listdir(class_path) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        print(f\"[{class_idx+1}/{num_classes}] Processing class '{class_name}': {len(image_files)} images\", end=' ')\n",
    "        \n",
    "        processed = 0\n",
    "        if use_tqdm:\n",
    "            iterator = tqdm(image_files, desc=f\"Class {class_name}\", leave=False)\n",
    "        else:\n",
    "            iterator = image_files\n",
    "        \n",
    "        for img_file in iterator:\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            landmarks = extract_landmarks(img_path)\n",
    "            \n",
    "            if landmarks is not None:\n",
    "                X_data.append(landmarks)\n",
    "                y_labels.append(class_name)\n",
    "                processed += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        \n",
    "        print(f\"‚Üí {processed} successful, {len(image_files)-processed} skipped\")\n",
    "\n",
    "hands.close()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_data = np.array(X_data, dtype=np.float32)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "skip_ratio = skipped_count / max(total_images, 1)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Dataset loaded: {len(X_data)} samples\")\n",
    "print(f\"‚úì Feature shape: {X_data.shape}\")\n",
    "print(f\"‚úì Skipped (no hand detected): {skipped_count} ({skip_ratio:.1%})\")\n",
    "if skip_ratio > 0.2:\n",
    "    print('‚ö† WARNING: Over 20% images skipped - consider reviewing data quality')\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check for NaN or Inf values (data quality check)\n",
    "if np.isnan(X_data).any() or np.isinf(X_data).any():\n",
    "    print('‚ö† WARNING: NaN or Inf values detected in features!')\n",
    "    print('Removing problematic samples...')\n",
    "    valid_mask = ~(np.isnan(X_data).any(axis=1) | np.isinf(X_data).any(axis=1))\n",
    "    X_data = X_data[valid_mask]\n",
    "    y_labels = y_labels[valid_mask]\n",
    "    print(f'‚úì Cleaned dataset: {len(X_data)} samples remaining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LABEL ENCODING AND VERIFICATION\n",
    "# Critical: Verify labels are ASL letters, not Train/Test\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "print(\"Label Mapping (index ‚Üí class):\")\n",
    "print(\"=\"*40)\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_labels == label)\n",
    "    print(f\"{i:2d} ‚Üí {label:3s} ({count:5d} samples)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Verify no incorrect labels\n",
    "assert 'Train' not in label_encoder.classes_ and 'Test' not in label_encoder.classes_, \\\n",
    "    \"CRITICAL ERROR: Train/Test detected in labels!\"\n",
    "\n",
    "# CRITICAL: Update num_classes to match actual labels found after extraction\n",
    "# Some classes may have been lost during landmark extraction\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"\\n‚úì Training on {num_classes} classes (updated from label encoder)\")\n",
    "print(f\"‚úì Classes: {list(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53352b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: STRATIFIED TRAIN/VAL SPLIT WITH DATA AUGMENTATION\n",
    "# Stratified splits: 80% train, 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_encoded, \n",
    "    test_size=0.2, \n",
    "    stratify=y_encoded,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Input shape: {X_train.shape[1]} features\")\n",
    "\n",
    "# Verify label distribution\n",
    "unique_train_labels = sorted(set(y_train))\n",
    "print(f\"\\n‚úì Training on class indices: {unique_train_labels}\")\n",
    "print(f\"‚úì Expected: 0 to {num_classes-1}\")\n",
    "\n",
    "# Data augmentation: Gaussian noise + horizontal flip\n",
    "ENABLE_AUGMENTATION = True  # Set to False to disable augmentation\n",
    "\n",
    "if ENABLE_AUGMENTATION:\n",
    "    print('\\nApplying data augmentation...')\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    augmented_X, augmented_y = [], []\n",
    "    \n",
    "    for vec, label in zip(X_train, y_train):\n",
    "        # Augmentation 1: Add Gaussian noise (œÉ=0.01)\n",
    "        noise = vec + rng.normal(0, 0.01, size=vec.shape)\n",
    "        augmented_X.append(noise.astype(np.float32))\n",
    "        augmented_y.append(label)\n",
    "        \n",
    "        # Augmentation 2: Horizontal flip (mirror x-coordinates)\n",
    "        mirrored = vec.copy()\n",
    "        mirrored[0::3] *= -1  # Flip every 3rd element (x coordinates)\n",
    "        augmented_X.append(mirrored.astype(np.float32))\n",
    "        augmented_y.append(label)\n",
    "    \n",
    "    # Combine original + augmented\n",
    "    X_train = np.concatenate([X_train, np.stack(augmented_X)], axis=0)\n",
    "    y_train = np.concatenate([y_train, np.array(augmented_y)])\n",
    "    \n",
    "    print(f'‚úì Augmentation complete: Train set expanded to {len(X_train):,} samples')\n",
    "else:\n",
    "    print('‚ö† Augmentation disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. MODEL ARCHITECTURE\n",
    "# Critical: Output layer size must match num_classes (dynamically set)\n",
    "def create_model(input_shape=63, num_classes=24):\n",
    "    \"\"\"Landmark-based classifier with dynamic output size.\n",
    "    \n",
    "    Best practices applied:\n",
    "    - BatchNormalization for stable training\n",
    "    - Dropout for regularization\n",
    "    - He initialization for ReLU layers\n",
    "    - L2 regularization to prevent overfitting\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        \n",
    "        # First dense block\n",
    "        layers.Dense(256, activation='relu', \n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Second dense block\n",
    "        layers.Dense(128, activation='relu',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Third dense block (optional, improves capacity)\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_initializer='he_normal'),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output layer - DYNAMIC SIZE matching num_classes\n",
    "        # Use dtype='float32' to ensure compatibility with mixed precision\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile model with correct output size\n",
    "model = create_model(input_shape=63, num_classes=num_classes)\n",
    "\n",
    "# Use Adam optimizer with learning rate schedule\n",
    "# Start with higher LR, reduce during training via callback\n",
    "initial_learning_rate = 1e-3\n",
    "optimizer = keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Verify output layer\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Model output shape: {model.output_shape}\")\n",
    "print(f\"‚úì Expected: (None, {num_classes})\")\n",
    "print(f\"‚úì Output units match num_classes: {model.layers[-1].units == num_classes}\")\n",
    "print(f\"‚úì Total trainable parameters: {model.count_params():,}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: CONFIGURE TRAINING CALLBACKS\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, \"best_model.keras\")\n",
    "\n",
    "# Checkpoint callback - saves best model based on validation accuracy\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    BEST_MODEL_PATH,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping - stop training if no improvement\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,  # Increased patience for better convergence\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate reduction - reduce LR when plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce LR by half\n",
    "    patience=5,   # Wait 5 epochs before reducing\n",
    "    min_lr=1e-7,  # Don't reduce below this value\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TensorBoard callback for visualization in Colab\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# Custom callback to clear output and show progress (Colab-friendly)\n",
    "class ColabProgressCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f\"Epoch {epoch+1}: \" +\n",
    "              f\"loss={logs.get('loss', 0):.4f}, \" +\n",
    "              f\"accuracy={logs.get('accuracy', 0):.4f}, \" +\n",
    "              f\"val_loss={logs.get('val_loss', 0):.4f}, \" +\n",
    "              f\"val_accuracy={logs.get('val_accuracy', 0):.4f}\")\n",
    "\n",
    "# Adjust batch size based on available hardware\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    BATCH_SIZE = 64  # GPU can handle larger batches\n",
    "else:\n",
    "    BATCH_SIZE = 32  # CPU works better with smaller batches\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE} ({'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'} optimized)\")\n",
    "print(f\"  ‚Ä¢ Max epochs: {EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Early stopping patience: 10 epochs\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {initial_learning_rate} (with reduction on plateau)\")\n",
    "print(f\"  ‚Ä¢ Callbacks: Checkpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\")\n",
    "\n",
    "print(f\"\\n‚úì Best model will be saved to: {BEST_MODEL_PATH}\")\n",
    "print(f\"‚úì TensorBoard logs: {os.path.join(OUTPUT_DIR, 'logs')}\")\n",
    "print(f\"\\nTo view TensorBoard in Colab, run:\")\n",
    "print(f\"  %load_ext tensorboard\")\n",
    "print(f\"  %tensorboard --logdir {os.path.join(OUTPUT_DIR, 'logs')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dda8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: TRAIN MODEL\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks_list = [\n",
    "    checkpoint_cb, \n",
    "    early_stopping, \n",
    "    reduce_lr, \n",
    "    tensorboard_cb,\n",
    "    ColabProgressCallback()\n",
    "]\n",
    "\n",
    "# Train model with verbose=2 for cleaner Colab output\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=2  # Less verbose, better for Colab\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Training complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Print training summary\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(f\"\\nFinal Training Metrics:\")\n",
    "print(f\"  ‚Ä¢ Train Loss: {final_train_loss:.4f}\")\n",
    "print(f\"  ‚Ä¢ Train Accuracy: {final_train_acc*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Val Loss: {final_val_loss:.4f}\")\n",
    "print(f\"  ‚Ä¢ Val Accuracy: {final_val_acc*100:.2f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "if final_train_acc - final_val_acc > 0.1:\n",
    "    print(f\"\\n‚ö† Warning: Possible overfitting detected!\")\n",
    "    print(f\"  Train-Val accuracy gap: {(final_train_acc - final_val_acc)*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Model generalization looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c934072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. EVALUATE MODEL\n",
    "print(\"\\nEvaluating model on validation set...\")\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "# Sample predictions check with confidence scores\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample Predictions (first 10):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_preds = model.predict(X_val[:10], verbose=0)\n",
    "print(f\"Prediction shape: {sample_preds.shape} (Expected: (10, {num_classes}))\")\n",
    "assert sample_preds.shape == (10, num_classes), \"Output shape mismatch!\"\n",
    "\n",
    "for i in range(10):\n",
    "    pred_class = np.argmax(sample_preds[i])\n",
    "    pred_label = label_encoder.classes_[pred_class]\n",
    "    true_label = label_encoder.classes_[y_val[i]]\n",
    "    confidence = sample_preds[i][pred_class] * 100\n",
    "    \n",
    "    # Color code: green for correct, red for incorrect\n",
    "    status = \"‚úì\" if pred_label == true_label else \"‚úó\"\n",
    "    print(f\"{i+1:2d}. True: {true_label:3s} | Pred: {pred_label:3s} ({confidence:5.1f}%) {status}\")\n",
    "\n",
    "# Calculate confusion matrix for detailed analysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating Classification Report...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Predict all validation samples\n",
    "all_preds = model.predict(X_val, verbose=0)\n",
    "pred_classes = np.argmax(all_preds, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(\n",
    "    y_val, \n",
    "    pred_classes, \n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=3\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix visualization\n",
    "print(\"\\nGenerating Confusion Matrix...\")\n",
    "cm = confusion_matrix(y_val, pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - ASL Classifier')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. PLOT TRAINING HISTORY\n",
    "print(\"\\nGenerating training history plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy over epochs\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss over epochs\n",
    "axes[0, 1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning rate over epochs (if ReduceLROnPlateau was triggered)\n",
    "if 'lr' in history.history:\n",
    "    axes[1, 0].plot(history.history['lr'], linewidth=2, color='orange')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Learning Rate history not available', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Per-class accuracy (if available)\n",
    "# Show final validation accuracy per class\n",
    "# Calculate accuracy for each class (handle empty classes)\n",
    "per_class_acc = []\n",
    "class_labels_present = []\n",
    "\n",
    "for i, class_label in enumerate(label_encoder.classes_):\n",
    "    mask = y_val == i\n",
    "    if mask.sum() > 0:  # Only include classes present in validation set\n",
    "        acc = np.mean(pred_classes[mask] == i)\n",
    "        per_class_acc.append(acc)\n",
    "        class_labels_present.append(class_label)\n",
    "\n",
    "# Plot only classes present in validation set\n",
    "x_positions = range(len(class_labels_present))\n",
    "axes[1, 1].bar(x_positions, per_class_acc, color='skyblue', edgecolor='navy')\n",
    "axes[1, 1].set_title('Per-Class Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Class', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 1].set_xticks(x_positions)\n",
    "axes[1, 1].set_xticklabels(class_labels_present, rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úì Training plots saved to: {os.path.join(OUTPUT_DIR, 'training_history.png')}\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Summary Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Final learning rate: {history.history.get('lr', [initial_learning_rate])[-1]:.2e}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 12: SAVE MODEL AND LABELS\n",
    "# Load best model from training\n",
    "best_model = keras.models.load_model(BEST_MODEL_PATH)\n",
    "print(f\"‚úì Loaded best model from: {BEST_MODEL_PATH}\")\n",
    "\n",
    "# Save labels.json (array format)\n",
    "labels_path = os.path.join(OUTPUT_DIR, \"labels.json\")\n",
    "with open(labels_path, 'w') as f:\n",
    "    json.dump(label_encoder.classes_.tolist(), f, indent=2)\n",
    "print(f\"‚úì Labels saved: {labels_path}\")\n",
    "\n",
    "# Verify labels.json content\n",
    "with open(labels_path, 'r') as f:\n",
    "    saved_labels = json.load(f)\n",
    "print(f\"\\n‚úì Verification: {len(saved_labels)} classes\")\n",
    "print(f\"  Labels: {saved_labels}\")\n",
    "\n",
    "# Verify label count matches the encoder (use actual encoder classes count)\n",
    "actual_num_classes = len(label_encoder.classes_)\n",
    "if len(saved_labels) != actual_num_classes:\n",
    "    print(f\"‚ö† Warning: saved_labels ({len(saved_labels)}) != label_encoder.classes_ ({actual_num_classes})\")\n",
    "    print(f\"  This should not happen. Re-saving from encoder...\")\n",
    "    with open(labels_path, 'w') as f:\n",
    "        json.dump(label_encoder.classes_.tolist(), f, indent=2)\n",
    "    print(f\"‚úì Labels re-saved from encoder\")\n",
    "\n",
    "assert 'Train' not in saved_labels and 'Test' not in saved_labels, \"Invalid labels detected!\"\n",
    "print(f\"‚úì Labels are correct! ({len(saved_labels)} classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12024e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 13: CONVERT TO TENSORFLOW.JS\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "tfjs_output_dir = os.path.join(OUTPUT_DIR, \"tfjs_model\")\n",
    "\n",
    "# Clean up old TFJS directory\n",
    "if os.path.exists(tfjs_output_dir):\n",
    "    shutil.rmtree(tfjs_output_dir)\n",
    "    print('‚úì Cleaned old TFJS directory')\n",
    "\n",
    "os.makedirs(tfjs_output_dir, exist_ok=True)\n",
    "\n",
    "print('\\nConverting to TensorFlow.js format...')\n",
    "print('This may take 1-2 minutes...\\n')\n",
    "\n",
    "try:\n",
    "    # Upgrade tensorflowjs to a version compatible with NumPy 1.26+\n",
    "    print('‚ö† Upgrading TensorFlow.js to latest version...')\n",
    "    !pip install -q --upgrade tensorflowjs\n",
    "    print('‚úì TensorFlow.js upgraded\\n')\n",
    "    \n",
    "    # Use Python API instead of command-line tool to avoid NumPy compatibility issues\n",
    "    import tensorflowjs as tfjs\n",
    "    \n",
    "    print('Using TensorFlow.js Python API for conversion...')\n",
    "    \n",
    "    # Convert using Python API - updated for latest tensorflowjs version\n",
    "    # The newer API has simplified parameters\n",
    "    tfjs.converters.save_keras_model(\n",
    "        best_model,\n",
    "        tfjs_output_dir\n",
    "    )\n",
    "    \n",
    "    print('‚úì TensorFlow.js conversion complete!')\n",
    "    \n",
    "    # Copy labels.json to tfjs directory\n",
    "    tfjs_labels_path = os.path.join(tfjs_output_dir, \"labels.json\")\n",
    "    shutil.copy(labels_path, tfjs_labels_path)\n",
    "    print(f\"‚úì Labels copied to: {tfjs_labels_path}\")\n",
    "    \n",
    "    # List generated files with sizes\n",
    "    print(f'\\nüì¶ Generated TFJS artifacts:')\n",
    "    tfjs_files = sorted(os.listdir(tfjs_output_dir))\n",
    "    total_size = 0\n",
    "    \n",
    "    for item in tfjs_files:\n",
    "        item_path = os.path.join(tfjs_output_dir, item)\n",
    "        size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f'   ‚Ä¢ {item:<30} {size_mb:>8.2f} MB')\n",
    "    \n",
    "    print(f'\\n   Total TFJS model size: {total_size:.2f} MB')\n",
    "    \n",
    "    # Create usage example file for web deployment\n",
    "    usage_example = f\"\"\"\n",
    "// TensorFlow.js Model Usage Example\n",
    "// Generated: {os.path.basename(OUTPUT_DIR)}\n",
    "\n",
    "// Load the model\n",
    "const model = await tf.loadGraphModel('path/to/tfjs_model/model.json');\n",
    "\n",
    "// Load labels\n",
    "const response = await fetch('path/to/tfjs_model/labels.json');\n",
    "const labels = await response.json();\n",
    "\n",
    "// Make prediction\n",
    "const inputTensor = tf.tensor2d([[...63 landmark features...]]); // Shape: [1, 63]\n",
    "const prediction = model.predict(inputTensor);\n",
    "const classIndex = prediction.argMax(-1).dataSync()[0];\n",
    "const predictedLabel = labels[classIndex];\n",
    "\n",
    "console.log('Predicted class:', predictedLabel);\n",
    "\"\"\"\n",
    "    \n",
    "    usage_path = os.path.join(tfjs_output_dir, \"usage_example.js\")\n",
    "    with open(usage_path, 'w') as f:\n",
    "        f.write(usage_example)\n",
    "    print(f'\\n‚úì Usage example saved to: {usage_path}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå TensorFlow.js conversion failed!')\n",
    "    print(f'Error: {type(e).__name__}: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14: FINAL VERIFICATION - Critical checks before deployment\n",
    "print(\"Final Deployment Verification:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Model input shape\n",
    "input_shape = best_model.input_shape\n",
    "print(f\"‚úì Model input shape: {input_shape} (Expected: (None, 63))\")\n",
    "assert input_shape == (None, 63), \"Input shape mismatch!\"\n",
    "\n",
    "# Check 2: Model output shape\n",
    "output_shape = best_model.output_shape\n",
    "actual_num_classes = len(label_encoder.classes_)\n",
    "print(f\"‚úì Model output shape: {output_shape}\")\n",
    "print(f\"  Note: Model has {output_shape[1]} outputs, but only {actual_num_classes} classes have labels\")\n",
    "\n",
    "# Check if there's a mismatch (one class may have had no valid samples)\n",
    "if output_shape[1] != actual_num_classes:\n",
    "    print(f\"‚ö† WARNING: Model output ({output_shape[1]}) != Label count ({actual_num_classes})\")\n",
    "    print(f\"  This likely means one class had no valid landmark extractions\")\n",
    "    print(f\"  The model will still work, but output index {actual_num_classes} will be unused\")\n",
    "    # This is acceptable - the extra output won't be used\n",
    "else:\n",
    "    assert output_shape == (None, actual_num_classes), \"Output shape mismatch!\"\n",
    "\n",
    "# Check 3: Labels file exists and is correct\n",
    "with open(tfjs_labels_path, 'r') as f:\n",
    "    tfjs_labels = json.load(f)\n",
    "print(f\"‚úì Labels in TFJS: {len(tfjs_labels)} classes\")\n",
    "print(f\"  {tfjs_labels}\")\n",
    "assert len(tfjs_labels) == actual_num_classes, \"Label count mismatch with encoder!\"\n",
    "assert isinstance(tfjs_labels, list), \"Labels must be an array!\"\n",
    "\n",
    "# Check 4: No Train/Test labels\n",
    "assert 'Train' not in tfjs_labels and 'Test' not in tfjs_labels, \\\n",
    "    \"CRITICAL: Train/Test labels still present!\"\n",
    "print(f\"‚úì No 'Train'/'Test' labels detected\")\n",
    "\n",
    "# Check 5: Files exist\n",
    "tfjs_model_json = os.path.join(tfjs_output_dir, \"model.json\")\n",
    "assert os.path.exists(tfjs_model_json), \"model.json not found!\"\n",
    "print(f\"‚úì model.json exists\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüéâ MODEL READY FOR DEPLOYMENT!\")\n",
    "print(f\"\\nüìÇ All files saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Check for output size mismatch\n",
    "if best_model.output_shape[1] != len(label_encoder.classes_):\n",
    "    print(f\"\\n‚ö† IMPORTANT NOTE:\")\n",
    "    print(f\"  Model has {best_model.output_shape[1]} outputs, but only {len(label_encoder.classes_)} labels\")\n",
    "    print(f\"  One dataset class had no valid landmark extractions\")\n",
    "    print(f\"  The model works fine - just ignore the unused output index\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Download '{OUTPUT_DIR}' folder from Google Drive\")\n",
    "print(f\"2. Copy tfjs_model/ to your web app\")\n",
    "print(f\"3. Load: const model = await tf.loadGraphModel('path/to/model.json');\")\n",
    "print(f\"4. Load: const labels = await fetch('path/to/labels.json').then(r=>r.json());\")\n",
    "print(f\"5. Ensure web app extracts 63 landmarks and centers at wrist\")\n",
    "print(f\"6. Get prediction: const classIdx = tf.argMax(prediction, -1); // Use labels[classIdx]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0989b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 15: TEST END-TO-END PREDICTION & GENERATE TRAINING REPORT\n",
    "import datetime\n",
    "\n",
    "# Test prediction\n",
    "test_idx = np.random.randint(0, len(X_val))\n",
    "test_sample = X_val[test_idx:test_idx+1]\n",
    "test_label = label_encoder.classes_[y_val[test_idx]]\n",
    "\n",
    "prediction = best_model.predict(test_sample, verbose=0)\n",
    "pred_class = np.argmax(prediction[0])\n",
    "pred_label = label_encoder.classes_[pred_class]\n",
    "confidence = prediction[0][pred_class] * 100\n",
    "\n",
    "print(f\"End-to-End Test:\")\n",
    "print(f\"  Input: {test_sample.shape}, Output: {prediction.shape}\")\n",
    "print(f\"  True: {test_label} | Predicted: {pred_label} ({confidence:.1f}%)\")\n",
    "print(f\"  Match: {'‚úì' if test_label == pred_label else '‚úó'}\")\n",
    "\n",
    "# Top 3 predictions\n",
    "top3_indices = np.argsort(prediction[0])[-3:][::-1]\n",
    "print(f\"\\n  Top 3 predictions:\")\n",
    "for i, idx in enumerate(top3_indices, 1):\n",
    "    label = label_encoder.classes_[idx]\n",
    "    conf = prediction[0][idx] * 100\n",
    "    print(f\"    {i}. {label}: {conf:.1f}%\")\n",
    "\n",
    "# Generate comprehensive training report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print('GENERATING TRAINING REPORT...')\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "report_content = f\"\"\"\n",
    "# ASL Sign Language Model Training Report\n",
    "Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Environment Information\n",
    "- TensorFlow Version: {tf.__version__}\n",
    "- MediaPipe Version: {mp.__version__}\n",
    "- Python Version: {sys.version}\n",
    "- GPU Available: {len(tf.config.list_physical_devices('GPU'))} GPU(s)\n",
    "- GPU Devices: {tf.config.list_physical_devices('GPU')}\n",
    "\n",
    "## Dataset Information\n",
    "- Total Classes: {num_classes}\n",
    "- Classes: {', '.join(label_encoder.classes_)}\n",
    "- Total Images Processed: {total_images:,}\n",
    "- Successful Extractions: {len(X_data):,}\n",
    "- Skipped Images: {skipped_count:,} ({skip_ratio:.1%})\n",
    "- Feature Dimensions: {X_data.shape[1]}\n",
    "\n",
    "## Data Split\n",
    "- Training Samples: {len(X_train):,} ({len(X_train)/len(X_data)*100:.1f}%)\n",
    "- Validation Samples: {len(X_val):,} ({len(X_val)/len(X_data)*100:.1f}%)\n",
    "- Augmentation Applied: {ENABLE_AUGMENTATION}\n",
    "\n",
    "## Model Architecture\n",
    "- Model Type: Sequential MLP Classifier\n",
    "- Input Shape: ({X_data.shape[1]},)\n",
    "- Total Parameters: {model.count_params():,}\n",
    "- Trainable Parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\n",
    "- Layer Configuration:\n",
    "  * Dense(256) + BatchNorm + Dropout(0.3)\n",
    "  * Dense(128) + BatchNorm + Dropout(0.2)\n",
    "  * Dense(64) + Dropout(0.1)\n",
    "  * Dense({num_classes}, softmax)\n",
    "\n",
    "## Training Configuration\n",
    "- Optimizer: Adam\n",
    "- Initial Learning Rate: {initial_learning_rate}\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Max Epochs: {EPOCHS}\n",
    "- Epochs Trained: {len(history.history['loss'])}\n",
    "- Early Stopping Patience: 10\n",
    "- Learning Rate Reduction: ReduceLROnPlateau (factor=0.5, patience=5)\n",
    "\n",
    "## Performance Metrics\n",
    "### Training Results\n",
    "- Final Train Loss: {final_train_loss:.4f}\n",
    "- Final Train Accuracy: {final_train_acc*100:.2f}%\n",
    "- Final Val Loss: {final_val_loss:.4f}\n",
    "- Final Val Accuracy: {final_val_acc*100:.2f}%\n",
    "\n",
    "### Best Results\n",
    "- Best Validation Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\n",
    "- Best Validation Loss: {min(history.history['val_loss']):.4f}\n",
    "\n",
    "### Model Generalization\n",
    "- Train-Val Accuracy Gap: {(final_train_acc - final_val_acc)*100:.2f}%\n",
    "- Overfitting Status: {'Possible overfitting' if final_train_acc - final_val_acc > 0.1 else 'Good generalization'}\n",
    "\n",
    "## Output Files\n",
    "- Best Model: {BEST_MODEL_PATH}\n",
    "- Labels File: {labels_path}\n",
    "- TFJS Model: {tfjs_output_dir}\n",
    "- Training Plots: {os.path.join(OUTPUT_DIR, 'training_history.png')}\n",
    "- TensorBoard Logs: {os.path.join(OUTPUT_DIR, 'logs')}\n",
    "\n",
    "## Deployment Instructions\n",
    "1. Download the entire output directory: {OUTPUT_DIR}\n",
    "2. For web deployment:\n",
    "   - Use model.json from tfjs_model/\n",
    "   - Load labels from tfjs_model/labels.json\n",
    "   - Input: 63 MediaPipe hand landmarks (wrist-centered)\n",
    "   - Output: Probability distribution over {num_classes} classes\n",
    "3. Recommended: Implement smoothing (5-frame majority voting) for real-time predictions\n",
    "\n",
    "## Notes\n",
    "- Model uses wrist-centered MediaPipe landmarks (21 landmarks √ó 3 coordinates = 63 features)\n",
    "- Labels format: Array of strings (not dictionary)\n",
    "- Model expects normalized landmark coordinates (0-1 range)\n",
    "- Recommended inference: Extract landmarks ‚Üí Center at wrist ‚Üí Predict\n",
    "\n",
    "---\n",
    "Report generated by ASL Model Training Pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_path = os.path.join(OUTPUT_DIR, 'training_report.md')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"\\n‚úì Training report saved to: {report_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print('TRAINING COMPLETE - ALL ARTIFACTS SAVED!')\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚Ä¢ Model: {BEST_MODEL_PATH}\")\n",
    "print(f\"  ‚Ä¢ Labels: {labels_path}\")\n",
    "print(f\"  ‚Ä¢ TFJS Model: {tfjs_output_dir}\")\n",
    "print(f\"  ‚Ä¢ Training Report: {report_path}\")\n",
    "print(f\"  ‚Ä¢ Classes: {num_classes}\")\n",
    "print(f\"  ‚Ä¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"  ‚Ä¢ Validation samples: {len(X_val):,}\")\n",
    "print(f\"  ‚Ä¢ Best Val Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"\\n‚úÖ All artifacts saved to Google Drive!\")\n",
    "print(f\"üì• Download from: {OUTPUT_DIR}\")\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"  1. Review training_report.md for detailed metrics\")\n",
    "print(f\"  2. Check confusion matrix for per-class performance\")\n",
    "print(f\"  3. Download all files from Google Drive\")\n",
    "print(f\"  4. Deploy tfjs_model/ to your web application\")\n",
    "print(f\"  5. Use usage_example.js as reference for integration\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
