{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6313d1",
   "metadata": {},
   "source": [
    "# üöÄ Improved ASL Alphabet Recognition - Hybrid CNN + Feature Engineering\n",
    "\n",
    "This notebook implements a **high-accuracy ASL alphabet recognition model** using:\n",
    "- **MediaPipe Hands** for landmark extraction (21 keypoints √ó 3D = 63 features)\n",
    "- **Engineered distance features** (distances between key finger points)\n",
    "- **Angle features** between finger joints\n",
    "- **1D CNN architecture** to capture spatial relationships\n",
    "- **Advanced data augmentation** for better generalization\n",
    "\n",
    "**Expected Accuracy: 97-99%** based on research findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies (Colab-compatible)\n",
    "print('Installing dependencies...')\n",
    "\n",
    "# Don't uninstall TensorFlow in Colab - use the pre-installed version\n",
    "# Only install additional packages we need\n",
    "\n",
    "# Install MediaPipe (separate to avoid conflicts)\n",
    "!pip install -q mediapipe>=0.10.14\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q \\\n",
    "    kagglehub \\\n",
    "    scikit-learn \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    tqdm \\\n",
    "    opencv-python-headless\n",
    "\n",
    "# Install TensorFlowJS converter (must match TF version)\n",
    "!pip install -q tensorflowjs\n",
    "\n",
    "print('\\n‚úì Installation complete!')\n",
    "\n",
    "# Verify versions\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nPython: {sys.version}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"MediaPipe: {mp.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\n{'‚úì GPU Available: ' + str(len(gpus)) + ' GPU(s)' if gpus else '‚ö† No GPU - using CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setup paths and mount Drive\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print('‚úì Google Drive mounted')\n",
    "except:\n",
    "    print('‚ö† Running locally - Drive not mounted')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "print(f'‚úì Random seed: {SEED}')\n",
    "\n",
    "# Paths\n",
    "DATASET_KAGGLE = 'kapillondhe/american-sign-language'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/sign_language_improved'\n",
    "RAW_LANDMARKS_NPZ = os.path.join(OUTPUT_DIR, 'landmarks_raw.npz')\n",
    "FEATURES_NPZ = os.path.join(OUTPUT_DIR, 'features_engineered.npz')\n",
    "PROC_NPZ = os.path.join(OUTPUT_DIR, 'data_processed.npz')\n",
    "BEST_KERAS = os.path.join(OUTPUT_DIR, 'best_model_improved.keras')\n",
    "SAVED_MODEL_DIR = os.path.join(OUTPUT_DIR, 'saved_model')\n",
    "TFJS_DIR = os.path.join(OUTPUT_DIR, 'tfjs_model')\n",
    "LABELS_JSON = os.path.join(OUTPUT_DIR, 'labels.json')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'‚úì Output directory: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download and validate dataset\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "print('Downloading ASL dataset from Kaggle...')\n",
    "path = kagglehub.dataset_download(DATASET_KAGGLE)\n",
    "print(f'‚úì Downloaded to: {path}')\n",
    "\n",
    "dataset_root = Path(path)\n",
    "\n",
    "# Handle nested folder structures\n",
    "if (dataset_root / 'ASL_Dataset').exists():\n",
    "    dataset_root = dataset_root / 'ASL_Dataset'\n",
    "\n",
    "# Check if dataset has Train/Test split structure\n",
    "if (dataset_root / 'Train').exists():\n",
    "    print('‚úì Found Train/Test split structure - using Train folder')\n",
    "    dataset_root = dataset_root / 'Train'\n",
    "elif (dataset_root / 'train').exists():\n",
    "    print('‚úì Found train/test split structure - using train folder')\n",
    "    dataset_root = dataset_root / 'train'\n",
    "\n",
    "# Get class directories (should be A-Z letters)\n",
    "class_dirs = sorted([p for p in dataset_root.iterdir() if p.is_dir()])\n",
    "class_names = [p.name for p in class_dirs]\n",
    "\n",
    "print(f'\\n‚úì Found {len(class_dirs)} classes: {class_names}')\n",
    "\n",
    "# Count images\n",
    "total_images = sum(len(list(d.glob('**/*.*'))) for d in class_dirs)\n",
    "print(f'‚úì Total images: {total_images:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering Functions\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# MediaPipe hand landmark indices\n",
    "# 0: WRIST\n",
    "# 1-4: THUMB (CMC, MCP, IP, TIP)\n",
    "# 5-8: INDEX (MCP, PIP, DIP, TIP)\n",
    "# 9-12: MIDDLE (MCP, PIP, DIP, TIP)\n",
    "# 13-16: RING (MCP, PIP, DIP, TIP)\n",
    "# 17-20: PINKY (MCP, PIP, DIP, TIP)\n",
    "\n",
    "FINGERTIP_IDS = [4, 8, 12, 16, 20]  # Thumb, Index, Middle, Ring, Pinky tips\n",
    "FINGER_MCP_IDS = [1, 5, 9, 13, 17]  # Base of each finger\n",
    "FINGER_PIP_IDS = [2, 6, 10, 14, 18]  # Second joint\n",
    "FINGER_DIP_IDS = [3, 7, 11, 15, 19]  # Third joint\n",
    "\n",
    "def calculate_distance(p1, p2):\n",
    "    \"\"\"Calculate Euclidean distance between two 3D points.\"\"\"\n",
    "    return np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    \"\"\"Calculate angle at p2 formed by p1-p2-p3 in degrees.\"\"\"\n",
    "    v1 = p1 - p2\n",
    "    v2 = p3 - p2\n",
    "    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)\n",
    "    cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cos_angle))\n",
    "\n",
    "def extract_engineered_features(landmarks):\n",
    "    \"\"\"\n",
    "    Extract engineered features from 21 hand landmarks.\n",
    "    \n",
    "    Args:\n",
    "        landmarks: (21, 3) array of x, y, z coordinates\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with different feature sets\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Wrist-centered normalized landmarks (original approach)\n",
    "    wrist = landmarks[0]\n",
    "    centered = landmarks - wrist\n",
    "    \n",
    "    # Normalize by hand size (distance from wrist to middle finger MCP)\n",
    "    hand_size = calculate_distance(landmarks[0], landmarks[9])\n",
    "    if hand_size > 0:\n",
    "        normalized = centered / hand_size\n",
    "    else:\n",
    "        normalized = centered\n",
    "    \n",
    "    features['normalized_landmarks'] = normalized.flatten()  # 63 features\n",
    "    \n",
    "    # 2. Distance features (key distances between landmarks)\n",
    "    distance_pairs = [\n",
    "        # Fingertips to wrist\n",
    "        (4, 0), (8, 0), (12, 0), (16, 0), (20, 0),\n",
    "        # Between adjacent fingertips\n",
    "        (4, 8), (8, 12), (12, 16), (16, 20),\n",
    "        # Thumb to other fingertips\n",
    "        (4, 8), (4, 12), (4, 16), (4, 20),\n",
    "        # Fingertips to palm center (approximated by middle MCP)\n",
    "        (4, 9), (8, 9), (12, 9), (16, 9), (20, 9),\n",
    "        # Between finger MCPs\n",
    "        (5, 9), (9, 13), (13, 17),\n",
    "        # Finger curl indicators (tip to MCP)\n",
    "        (4, 1), (8, 5), (12, 9), (16, 13), (20, 17),\n",
    "    ]\n",
    "    \n",
    "    distances = []\n",
    "    for i, j in distance_pairs:\n",
    "        d = calculate_distance(normalized[i], normalized[j])\n",
    "        distances.append(d)\n",
    "    \n",
    "    features['distances'] = np.array(distances, dtype=np.float32)  # 24 features\n",
    "    \n",
    "    # 3. Angle features (joint angles)\n",
    "    angles = []\n",
    "    \n",
    "    # Finger joint angles (PIP angles - how bent each finger is)\n",
    "    for finger_base, pip, dip, tip in [(1,2,3,4), (5,6,7,8), (9,10,11,12), (13,14,15,16), (17,18,19,20)]:\n",
    "        # MCP angle\n",
    "        angles.append(calculate_angle(landmarks[0], landmarks[finger_base], landmarks[pip]))\n",
    "        # PIP angle\n",
    "        angles.append(calculate_angle(landmarks[finger_base], landmarks[pip], landmarks[dip]))\n",
    "        # DIP angle\n",
    "        angles.append(calculate_angle(landmarks[pip], landmarks[dip], landmarks[tip]))\n",
    "    \n",
    "    # Angles between fingers (spread)\n",
    "    for i in range(len(FINGERTIP_IDS) - 1):\n",
    "        tip1 = FINGERTIP_IDS[i]\n",
    "        tip2 = FINGERTIP_IDS[i + 1]\n",
    "        mcp = FINGER_MCP_IDS[i + 1]\n",
    "        angles.append(calculate_angle(landmarks[tip1], landmarks[0], landmarks[tip2]))\n",
    "    \n",
    "    features['angles'] = np.array(angles, dtype=np.float32) / 180.0  # Normalize to [0,1], 19 features\n",
    "    \n",
    "    # 4. Fingertip positions relative to wrist (height/depth features)\n",
    "    fingertip_heights = []\n",
    "    for tip_id in FINGERTIP_IDS:\n",
    "        # Y-coordinate (height) relative to wrist\n",
    "        fingertip_heights.append(normalized[tip_id][1])\n",
    "        # Z-coordinate (depth) relative to wrist\n",
    "        fingertip_heights.append(normalized[tip_id][2])\n",
    "    \n",
    "    features['fingertip_positions'] = np.array(fingertip_heights, dtype=np.float32)  # 10 features\n",
    "    \n",
    "    # 5. Hand orientation features\n",
    "    # Palm normal approximation using cross product of vectors\n",
    "    v1 = landmarks[5] - landmarks[0]  # Wrist to index MCP\n",
    "    v2 = landmarks[17] - landmarks[0]  # Wrist to pinky MCP\n",
    "    palm_normal = np.cross(v1, v2)\n",
    "    palm_normal = palm_normal / (np.linalg.norm(palm_normal) + 1e-8)\n",
    "    \n",
    "    features['orientation'] = palm_normal.astype(np.float32)  # 3 features\n",
    "    \n",
    "    return features\n",
    "\n",
    "def combine_features(features_dict):\n",
    "    \"\"\"\n",
    "    Combine all feature sets into a single vector.\n",
    "    \n",
    "    Total features: 63 + 24 + 19 + 10 + 3 = 119 features\n",
    "    \"\"\"\n",
    "    return np.concatenate([\n",
    "        features_dict['normalized_landmarks'],\n",
    "        features_dict['distances'],\n",
    "        features_dict['angles'],\n",
    "        features_dict['fingertip_positions'],\n",
    "        features_dict['orientation']\n",
    "    ])\n",
    "\n",
    "print('‚úì Feature engineering functions defined')\n",
    "print(f'  - Normalized landmarks: 63 features')\n",
    "print(f'  - Distance features: 24 features')\n",
    "print(f'  - Angle features: 19 features')\n",
    "print(f'  - Fingertip positions: 10 features')\n",
    "print(f'  - Orientation: 3 features')\n",
    "print(f'  - Total: 119 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract landmarks and engineered features from dataset\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# Import MediaPipe with compatibility for different versions\n",
    "try:\n",
    "    # Try legacy API first (works with older mediapipe)\n",
    "    import mediapipe as mp\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    USE_LEGACY_API = True\n",
    "    print('‚úì Using MediaPipe legacy API')\n",
    "except AttributeError:\n",
    "    # Use new Tasks API (mediapipe 0.10.14+)\n",
    "    import mediapipe as mp\n",
    "    from mediapipe.tasks import python\n",
    "    from mediapipe.tasks.python import vision\n",
    "    \n",
    "    # Download hand landmarker model\n",
    "    import urllib.request\n",
    "    import os\n",
    "    \n",
    "    model_path = '/content/hand_landmarker.task'\n",
    "    if not os.path.exists(model_path):\n",
    "        print('Downloading hand landmarker model...')\n",
    "        urllib.request.urlretrieve(\n",
    "            'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',\n",
    "            model_path\n",
    "        )\n",
    "    \n",
    "    base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "    options = vision.HandLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        num_hands=1,\n",
    "        min_hand_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    hands = vision.HandLandmarker.create_from_options(options)\n",
    "    USE_LEGACY_API = False\n",
    "    print('‚úì Using MediaPipe Tasks API')\n",
    "\n",
    "print('‚úì MediaPipe Hands initialized')\n",
    "\n",
    "labels = [d.name for d in class_dirs]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "all_landmarks = []  # Raw normalized landmarks (63 features)\n",
    "all_features = []   # Combined engineered features (119 features)\n",
    "all_labels = []\n",
    "class_counts = {label: 0 for label in labels}\n",
    "skipped = 0\n",
    "total_processed = 0\n",
    "\n",
    "print(f'\\nProcessing {len(class_dirs)} classes...\\n')\n",
    "\n",
    "for label_dir in class_dirs:\n",
    "    # Get all image files\n",
    "    image_files = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "        image_files.extend(list(label_dir.glob(f'**/{ext}')))\n",
    "    image_files = sorted(set(image_files))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f'‚ö† No images in {label_dir.name}')\n",
    "        continue\n",
    "    \n",
    "    total_processed += len(image_files)\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=f'{label_dir.name}', leave=False):\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process based on API version\n",
    "            if USE_LEGACY_API:\n",
    "                result = hands.process(rgb)\n",
    "                if not result.multi_hand_landmarks:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                lm = result.multi_hand_landmarks[0].landmark\n",
    "                landmarks_array = np.array([[p.x, p.y, p.z] for p in lm], dtype=np.float32)\n",
    "            else:\n",
    "                # Tasks API\n",
    "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "                result = hands.detect(mp_image)\n",
    "                if not result.hand_landmarks:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                lm = result.hand_landmarks[0]\n",
    "                landmarks_array = np.array([[p.x, p.y, p.z] for p in lm], dtype=np.float32)\n",
    "            \n",
    "            # Extract engineered features\n",
    "            features_dict = extract_engineered_features(landmarks_array)\n",
    "            combined_features = combine_features(features_dict)\n",
    "            \n",
    "            all_landmarks.append(features_dict['normalized_landmarks'])\n",
    "            all_features.append(combined_features)\n",
    "            all_labels.append(label_to_idx[label_dir.name])\n",
    "            class_counts[label_dir.name] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# Close hands detector if legacy API\n",
    "if USE_LEGACY_API:\n",
    "    hands.close()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_landmarks = np.stack(all_landmarks, dtype=np.float32)\n",
    "X_features = np.stack(all_features, dtype=np.float32)\n",
    "y = np.array(all_labels, dtype=np.int32)\n",
    "\n",
    "# Save raw data\n",
    "np.savez(\n",
    "    FEATURES_NPZ,\n",
    "    X_landmarks=X_landmarks,\n",
    "    X_features=X_features,\n",
    "    y=y,\n",
    "    labels=np.array(labels),\n",
    "    class_counts=np.array([class_counts[l] for l in labels])\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì Feature extraction complete!')\n",
    "print(f'  Total images: {total_processed:,}')\n",
    "print(f'  Successful: {len(X_features):,}')\n",
    "print(f'  Skipped: {skipped:,} ({skipped/total_processed*100:.1f}%)')\n",
    "print(f'\\n  Landmarks shape: {X_landmarks.shape}')\n",
    "print(f'  Features shape: {X_features.shape}')\n",
    "print(f'\\n‚úì Saved to: {FEATURES_NPZ}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Data Augmentation Functions\n",
    "import numpy as np\n",
    "\n",
    "def augment_landmarks(landmarks, features):\n",
    "    \"\"\"\n",
    "    Apply augmentation to landmarks and recalculate features.\n",
    "    \n",
    "    Args:\n",
    "        landmarks: (63,) normalized landmarks\n",
    "        features: (119,) combined features\n",
    "    \n",
    "    Returns:\n",
    "        List of augmented (landmarks, features) tuples\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    lm_reshaped = landmarks.reshape(21, 3)\n",
    "    \n",
    "    # 1. Add Gaussian noise (small perturbations)\n",
    "    for noise_level in [0.01, 0.02]:\n",
    "        noisy = lm_reshaped + np.random.normal(0, noise_level, lm_reshaped.shape)\n",
    "        noisy_features = extract_engineered_features(noisy)\n",
    "        augmented.append((\n",
    "            noisy_features['normalized_landmarks'],\n",
    "            combine_features(noisy_features)\n",
    "        ))\n",
    "    \n",
    "    # 2. Horizontal flip (mirror x-coordinates)\n",
    "    flipped = lm_reshaped.copy()\n",
    "    flipped[:, 0] *= -1  # Flip x\n",
    "    flipped_features = extract_engineered_features(flipped)\n",
    "    augmented.append((\n",
    "        flipped_features['normalized_landmarks'],\n",
    "        combine_features(flipped_features)\n",
    "    ))\n",
    "    \n",
    "    # 3. Small rotation around Z-axis (palm rotation)\n",
    "    for angle_deg in [5, -5, 10, -10]:\n",
    "        angle = np.radians(angle_deg)\n",
    "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "        rotation_matrix = np.array([\n",
    "            [cos_a, -sin_a, 0],\n",
    "            [sin_a, cos_a, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        rotated = lm_reshaped @ rotation_matrix.T\n",
    "        rotated_features = extract_engineered_features(rotated)\n",
    "        augmented.append((\n",
    "            rotated_features['normalized_landmarks'],\n",
    "            combine_features(rotated_features)\n",
    "        ))\n",
    "    \n",
    "    # 4. Scale variation\n",
    "    for scale in [0.9, 1.1]:\n",
    "        scaled = lm_reshaped * scale\n",
    "        scaled_features = extract_engineered_features(scaled)\n",
    "        augmented.append((\n",
    "            scaled_features['normalized_landmarks'],\n",
    "            combine_features(scaled_features)\n",
    "        ))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "print('‚úì Augmentation functions defined')\n",
    "print('  - Gaussian noise (2 levels)')\n",
    "print('  - Horizontal flip')\n",
    "print('  - Z-axis rotation (4 angles)')\n",
    "print('  - Scale variation (2 levels)')\n",
    "print('  - Total: 9 augmentations per sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create train/val/test splits with augmentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print('Loading extracted features...')\n",
    "data = np.load(FEATURES_NPZ, allow_pickle=True)\n",
    "X_landmarks = data['X_landmarks']\n",
    "X_features = data['X_features']\n",
    "y = data['y']\n",
    "labels = data['labels']\n",
    "\n",
    "print(f'‚úì Loaded {len(X_features):,} samples, {len(labels)} classes')\n",
    "\n",
    "# Stratified splits: 75% train, 15% val, 10% test\n",
    "X_lm_train, X_lm_temp, X_ft_train, X_ft_temp, y_train, y_temp = train_test_split(\n",
    "    X_landmarks, X_features, y, test_size=0.25, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_lm_val, X_lm_test, X_ft_val, X_ft_test, y_val, y_test = train_test_split(\n",
    "    X_lm_temp, X_ft_temp, y_temp, test_size=0.4, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f'\\nInitial splits:')\n",
    "print(f'  Train: {len(y_train):,}')\n",
    "print(f'  Val:   {len(y_val):,}')\n",
    "print(f'  Test:  {len(y_test):,}')\n",
    "\n",
    "# Apply augmentation to training data only\n",
    "ENABLE_AUGMENTATION = True\n",
    "AUG_SAMPLES_PER_ORIGINAL = 5  # Use 5 augmentations per sample (out of 9 available)\n",
    "\n",
    "if ENABLE_AUGMENTATION:\n",
    "    print(f'\\nAugmenting training data ({AUG_SAMPLES_PER_ORIGINAL}x)...')\n",
    "    aug_landmarks = []\n",
    "    aug_features = []\n",
    "    aug_labels = []\n",
    "    \n",
    "    for i in tqdm(range(len(X_lm_train)), desc='Augmenting'):\n",
    "        augmented = augment_landmarks(X_lm_train[i], X_ft_train[i])\n",
    "        # Randomly select augmentations\n",
    "        selected = np.random.choice(len(augmented), min(AUG_SAMPLES_PER_ORIGINAL, len(augmented)), replace=False)\n",
    "        for idx in selected:\n",
    "            lm, ft = augmented[idx]\n",
    "            aug_landmarks.append(lm)\n",
    "            aug_features.append(ft)\n",
    "            aug_labels.append(y_train[i])\n",
    "    \n",
    "    # Combine original + augmented\n",
    "    X_lm_train = np.concatenate([X_lm_train, np.array(aug_landmarks)])\n",
    "    X_ft_train = np.concatenate([X_ft_train, np.array(aug_features)])\n",
    "    y_train = np.concatenate([y_train, np.array(aug_labels)])\n",
    "    \n",
    "    print(f'‚úì Training set expanded to {len(y_train):,} samples')\n",
    "\n",
    "# Save processed data\n",
    "np.savez(\n",
    "    PROC_NPZ,\n",
    "    X_lm_train=X_lm_train, X_ft_train=X_ft_train, y_train=y_train,\n",
    "    X_lm_val=X_lm_val, X_ft_val=X_ft_val, y_val=y_val,\n",
    "    X_lm_test=X_lm_test, X_ft_test=X_ft_test, y_test=y_test,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì Saved to: {PROC_NPZ}')\n",
    "print(f'\\nFinal shapes:')\n",
    "print(f'  Train landmarks: {X_lm_train.shape}')\n",
    "print(f'  Train features:  {X_ft_train.shape}')\n",
    "print(f'  Val landmarks:   {X_lm_val.shape}')\n",
    "print(f'  Test landmarks:  {X_lm_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Build Hybrid CNN + MLP Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, regularizers\n",
    "\n",
    "print('Loading processed data...')\n",
    "proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "X_lm_train, X_ft_train, y_train = proc['X_lm_train'], proc['X_ft_train'], proc['y_train']\n",
    "X_lm_val, X_ft_val, y_val = proc['X_lm_val'], proc['X_ft_val'], proc['y_val']\n",
    "num_classes = len(proc['labels'])\n",
    "\n",
    "print(f'‚úì Train: {X_ft_train.shape}, Val: {X_ft_val.shape}')\n",
    "print(f'‚úì Classes: {num_classes}')\n",
    "\n",
    "# Model hyperparameters\n",
    "DROPOUT_RATE = 0.3\n",
    "L2_REG = 1e-4\n",
    "\n",
    "def build_hybrid_model(landmark_dim=63, feature_dim=119, num_classes=24):\n",
    "    \"\"\"\n",
    "    Hybrid model combining:\n",
    "    1. 1D CNN branch for landmark spatial patterns\n",
    "    2. Dense branch for engineered features\n",
    "    3. Concatenated for final classification\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== Branch 1: CNN on landmarks =====\n",
    "    landmark_input = layers.Input(shape=(landmark_dim,), name='landmark_input')\n",
    "    \n",
    "    # Reshape to (21 landmarks, 3 coordinates)\n",
    "    x1 = layers.Reshape((21, 3), name='reshape_landmarks')(landmark_input)\n",
    "    \n",
    "    # 1D CNN blocks\n",
    "    x1 = layers.Conv1D(64, 3, padding='same', activation='relu',\n",
    "                       kernel_regularizer=regularizers.l2(L2_REG), name='conv1')(x1)\n",
    "    x1 = layers.BatchNormalization(name='bn1')(x1)\n",
    "    \n",
    "    x1 = layers.Conv1D(128, 3, padding='same', activation='relu',\n",
    "                       kernel_regularizer=regularizers.l2(L2_REG), name='conv2')(x1)\n",
    "    x1 = layers.BatchNormalization(name='bn2')(x1)\n",
    "    x1 = layers.MaxPooling1D(2, name='pool1')(x1)\n",
    "    x1 = layers.Dropout(DROPOUT_RATE, name='dropout1')(x1)\n",
    "    \n",
    "    x1 = layers.Conv1D(256, 3, padding='same', activation='relu',\n",
    "                       kernel_regularizer=regularizers.l2(L2_REG), name='conv3')(x1)\n",
    "    x1 = layers.BatchNormalization(name='bn3')(x1)\n",
    "    \n",
    "    x1 = layers.Conv1D(256, 3, padding='same', activation='relu',\n",
    "                       kernel_regularizer=regularizers.l2(L2_REG), name='conv4')(x1)\n",
    "    x1 = layers.BatchNormalization(name='bn4')(x1)\n",
    "    x1 = layers.Dropout(DROPOUT_RATE, name='dropout2')(x1)\n",
    "    \n",
    "    # Global pooling\n",
    "    x1 = layers.GlobalAveragePooling1D(name='gap')(x1)\n",
    "    x1 = layers.Dense(128, activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(L2_REG), name='cnn_dense')(x1)\n",
    "    \n",
    "    # ===== Branch 2: Dense on engineered features =====\n",
    "    feature_input = layers.Input(shape=(feature_dim,), name='feature_input')\n",
    "    \n",
    "    x2 = layers.Dense(256, activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(L2_REG), name='feat_dense1')(feature_input)\n",
    "    x2 = layers.BatchNormalization(name='feat_bn1')(x2)\n",
    "    x2 = layers.Dropout(DROPOUT_RATE, name='feat_dropout1')(x2)\n",
    "    \n",
    "    x2 = layers.Dense(128, activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(L2_REG), name='feat_dense2')(x2)\n",
    "    x2 = layers.BatchNormalization(name='feat_bn2')(x2)\n",
    "    x2 = layers.Dropout(DROPOUT_RATE * 0.5, name='feat_dropout2')(x2)\n",
    "    \n",
    "    # ===== Merge branches =====\n",
    "    merged = layers.Concatenate(name='merge')([x1, x2])\n",
    "    \n",
    "    # Final classification layers\n",
    "    x = layers.Dense(256, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(L2_REG), name='final_dense1')(merged)\n",
    "    x = layers.BatchNormalization(name='final_bn1')(x)\n",
    "    x = layers.Dropout(DROPOUT_RATE, name='final_dropout1')(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(L2_REG), name='final_dense2')(x)\n",
    "    x = layers.Dropout(DROPOUT_RATE * 0.5, name='final_dropout2')(x)\n",
    "    \n",
    "    # Output\n",
    "    output = layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[landmark_input, feature_input],\n",
    "        outputs=output,\n",
    "        name='ASL_Hybrid_CNN'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_hybrid_model(\n",
    "    landmark_dim=X_lm_train.shape[1],\n",
    "    feature_dim=X_ft_train.shape[1],\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Compile with label smoothing for better generalization\n",
    "model.compile(\n",
    "    optimizer=optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('\\n‚úì Model built successfully!')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Train the model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    BEST_KERAS,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "earlystop_cb = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_cb = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate warmup scheduler (TF 2.16+ compatible)\n",
    "class WarmupScheduler(callbacks.Callback):\n",
    "    def __init__(self, warmup_epochs=5, initial_lr=1e-4, target_lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        self.target_lr = target_lr\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            lr = self.initial_lr + (self.target_lr - self.initial_lr) * (epoch / self.warmup_epochs)\n",
    "            # TF 2.16+ compatible way to set learning rate\n",
    "            self.model.optimizer.learning_rate.assign(lr)\n",
    "\n",
    "warmup_cb = WarmupScheduler(warmup_epochs=5, initial_lr=1e-4, target_lr=1e-3)\n",
    "\n",
    "print('Starting training...\\n')\n",
    "\n",
    "history = model.fit(\n",
    "    [X_lm_train, X_ft_train], y_train,\n",
    "    validation_data=([X_lm_val, X_ft_val], y_val),\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb, warmup_cb],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print('\\n‚úì Training complete!')\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n‚úì Best model saved: {BEST_KERAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9B: RECOVERY CELL - Resume training after disconnect\n",
    "# Run this INSTEAD of Step 9 if you were disconnected during training\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if not os.path.exists(BEST_KERAS):\n",
    "    print('‚ùå No checkpoint found! Run Step 9 (training) from the beginning.')\n",
    "else:\n",
    "    print('‚úì Found checkpoint, resuming training...\\n')\n",
    "    \n",
    "    # Load processed data\n",
    "    proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "    X_lm_train, X_ft_train, y_train = proc['X_lm_train'], proc['X_ft_train'], proc['y_train']\n",
    "    X_lm_val, X_ft_val, y_val = proc['X_lm_val'], proc['X_ft_val'], proc['y_val']\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    model = tf.keras.models.load_model(BEST_KERAS)\n",
    "    print(f'‚úì Loaded checkpoint: {BEST_KERAS}')\n",
    "    \n",
    "    # Get current best accuracy (evaluate on validation set)\n",
    "    _, current_val_acc = model.evaluate([X_lm_val, X_ft_val], y_val, verbose=0)\n",
    "    print(f'‚úì Current validation accuracy: {current_val_acc:.4f} ({current_val_acc*100:.2f}%)')\n",
    "    \n",
    "    # If already at high accuracy, skip further training\n",
    "    if current_val_acc >= 0.98:\n",
    "        print(f'\\nüéâ Model already at {current_val_acc*100:.2f}% accuracy!')\n",
    "        print('   Skipping additional training. Proceed to evaluation (Step 10).')\n",
    "    else:\n",
    "        # Continue training with reduced epochs\n",
    "        REMAINING_EPOCHS = 50  # Fewer epochs since we're resuming\n",
    "        \n",
    "        # Callbacks (same as original)\n",
    "        checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "            BEST_KERAS, monitor='val_accuracy', save_best_only=True, verbose=1\n",
    "        )\n",
    "        earlystop_cb = callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1\n",
    "        )\n",
    "        reduce_lr_cb = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f'\\nResuming training for up to {REMAINING_EPOCHS} more epochs...\\n')\n",
    "        \n",
    "        history = model.fit(\n",
    "            [X_lm_train, X_ft_train], y_train,\n",
    "            validation_data=([X_lm_val, X_ft_val], y_val),\n",
    "            epochs=REMAINING_EPOCHS,\n",
    "            batch_size=64,\n",
    "            callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        print('\\n‚úì Training resumed and complete!')\n",
    "        \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "        axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "        axes[0].set_title('Model Accuracy (Resumed)', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "        axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "        axes[1].set_title('Model Loss (Resumed)', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa20fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Evaluate on test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print('Loading test data and best model...')\n",
    "proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "X_lm_test, X_ft_test, y_test = proc['X_lm_test'], proc['X_ft_test'], proc['y_test']\n",
    "labels = proc['labels']\n",
    "\n",
    "best_model = tf.keras.models.load_model(BEST_KERAS)\n",
    "print('‚úì Model loaded')\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = best_model.evaluate([X_lm_test, X_ft_test], y_test, verbose=0)\n",
    "print(f'\\nüìä Test Set Performance:')\n",
    "print(f'   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'   Loss: {test_loss:.4f}')\n",
    "\n",
    "# Predictions\n",
    "y_probs = best_model.predict([X_lm_test, X_ft_test], verbose=0)\n",
    "y_pred = y_probs.argmax(axis=1)\n",
    "\n",
    "# Classification report - use labels parameter to handle missing classes\n",
    "print('\\n' + '='*70)\n",
    "print('CLASSIFICATION REPORT')\n",
    "print('='*70)\n",
    "# Convert labels to list of strings if needed\n",
    "label_names = [str(l) for l in labels]\n",
    "print(classification_report(y_test, y_pred, labels=range(len(labels)), target_names=label_names, digits=4, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=range(len(labels)))\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f30864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Export for TensorFlow.js (Browser deployment)\n",
    "# Since hybrid model has 2 inputs, we need to export both inputs separately\n",
    "# Feature computation will happen in JavaScript using featureEngineering.ts\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print('Creating model for browser deployment...\\n')\n",
    "\n",
    "# Clean up old exports\n",
    "for path in [SAVED_MODEL_DIR, TFJS_DIR]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "os.makedirs(TFJS_DIR, exist_ok=True)\n",
    "\n",
    "# Load best model and data to get feature dimensions\n",
    "best_model = tf.keras.models.load_model(BEST_KERAS)\n",
    "proc = np.load(PROC_NPZ, allow_pickle=True)\n",
    "\n",
    "# Get actual feature dimensions from training data\n",
    "landmark_dim = proc['X_lm_train'].shape[1]  # 63\n",
    "feature_dim = proc['X_ft_train'].shape[1]   # 121 (actual)\n",
    "labels = proc['labels']\n",
    "\n",
    "print(f'‚úì Landmark dimension: {landmark_dim}')\n",
    "print(f'‚úì Feature dimension: {feature_dim}')\n",
    "print(f'‚úì Classes: {len(labels)}')\n",
    "\n",
    "# For browser deployment, we have two options:\n",
    "# Option 1: Export the dual-input model directly (requires JS to compute features)\n",
    "# Option 2: Create a single-input wrapper that computes features in TF\n",
    "\n",
    "# We'll use Option 1 - export dual-input model, compute features in JavaScript\n",
    "# This gives more control and matches our featureEngineering.ts implementation\n",
    "\n",
    "# Save the model directly as SavedModel\n",
    "print(f'\\nExporting to SavedModel...')\n",
    "best_model.export(SAVED_MODEL_DIR)\n",
    "print(f'‚úì Saved: {SAVED_MODEL_DIR}')\n",
    "\n",
    "# Save labels\n",
    "label_map = {int(i): str(l) for i, l in enumerate(labels)}\n",
    "with open(LABELS_JSON, 'w') as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "print(f'‚úì Labels saved: {LABELS_JSON}')\n",
    "\n",
    "# Save feature dimensions for JS reference\n",
    "config = {\n",
    "    'landmark_dim': int(landmark_dim),\n",
    "    'feature_dim': int(feature_dim),\n",
    "    'num_classes': int(len(labels)),\n",
    "    'input_names': ['landmark_input', 'feature_input'],\n",
    "    'labels': label_map\n",
    "}\n",
    "config_path = os.path.join(OUTPUT_DIR, 'model_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f'‚úì Config saved: {config_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Convert to TensorFlow.js\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print('Converting to TensorFlow.js format...\\n')\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'tensorflowjs_converter',\n",
    "        '--input_format=tf_saved_model',\n",
    "        '--output_format=tfjs_graph_model',\n",
    "        '--signature_name=serving_default',\n",
    "        '--strip_debug_ops=True',\n",
    "        SAVED_MODEL_DIR,\n",
    "        TFJS_DIR\n",
    "    ], check=True, capture_output=True, text=True)\n",
    "    \n",
    "    print('‚úì TFJS conversion complete!')\n",
    "    \n",
    "    # List files\n",
    "    print('\\nüì¶ Generated files:')\n",
    "    total_size = 0\n",
    "    for f in sorted(os.listdir(TFJS_DIR)):\n",
    "        size = os.path.getsize(os.path.join(TFJS_DIR, f)) / 1024\n",
    "        total_size += size\n",
    "        print(f'   {f:<30} {size:>8.1f} KB')\n",
    "    print(f'\\n   Total: {total_size:.1f} KB ({total_size/1024:.2f} MB)')\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f'‚ùå Conversion error: {e.stderr}')\n",
    "    print('\\nTrying alternative conversion from Keras format...')\n",
    "    \n",
    "    # Try converting directly from .keras file\n",
    "    result = subprocess.run([\n",
    "        'tensorflowjs_converter',\n",
    "        '--input_format=keras',\n",
    "        '--output_format=tfjs_layers_model',\n",
    "        BEST_KERAS,\n",
    "        TFJS_DIR\n",
    "    ], check=True, capture_output=True, text=True)\n",
    "    \n",
    "    print('‚úì TFJS conversion complete (layers model format)!')\n",
    "    \n",
    "    # List files\n",
    "    print('\\nüì¶ Generated files:')\n",
    "    total_size = 0\n",
    "    for f in sorted(os.listdir(TFJS_DIR)):\n",
    "        size = os.path.getsize(os.path.join(TFJS_DIR, f)) / 1024\n",
    "        total_size += size\n",
    "        print(f'   {f:<30} {size:>8.1f} KB')\n",
    "    print(f'\\n   Total: {total_size:.1f} KB ({total_size/1024:.2f} MB)')\n",
    "\n",
    "# Copy labels and config to TFJS directory\n",
    "shutil.copy(LABELS_JSON, os.path.join(TFJS_DIR, 'labels.json'))\n",
    "config_src = os.path.join(OUTPUT_DIR, 'model_config.json')\n",
    "if os.path.exists(config_src):\n",
    "    shutil.copy(config_src, os.path.join(TFJS_DIR, 'model_config.json'))\n",
    "print(f'\\n‚úì labels.json and config copied to TFJS directory')\n",
    "\n",
    "print('\\nüéâ Model ready for browser deployment!')\n",
    "print(f'\\nFiles to deploy:')\n",
    "print(f'  üìÅ {TFJS_DIR}/')\n",
    "for f in sorted(os.listdir(TFJS_DIR)):\n",
    "    print(f'     - {f}')\n",
    "\n",
    "print('\\n‚ö†Ô∏è  IMPORTANT: This model has TWO inputs:')\n",
    "print('   1. landmark_input: (63,) - normalized landmarks')\n",
    "print('   2. feature_input: (121,) - engineered features')\n",
    "print('\\n   Use featureEngineering.ts to compute features in the browser.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664f716",
   "metadata": {},
   "source": [
    "# üéâ Training Complete!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Download the TFJS model files** from Google Drive:\n",
    "   - `model.json`\n",
    "   - `group1-shard*.bin` files\n",
    "   - `labels.json`\n",
    "\n",
    "2. **Copy to your project**:\n",
    "   ```\n",
    "   public/models/alphabet_tfjs/\n",
    "   ‚îú‚îÄ‚îÄ model.json\n",
    "   ‚îú‚îÄ‚îÄ group1-shard1of1.bin\n",
    "   ‚îî‚îÄ‚îÄ labels.json\n",
    "   ```\n",
    "\n",
    "3. **Update your app config** if needed to point to the new model.\n",
    "\n",
    "## Model Architecture Summary\n",
    "\n",
    "- **Input**: 63 features (21 landmarks √ó 3 coordinates)\n",
    "- **Branch 1**: 1D CNN (captures spatial patterns in hand structure)\n",
    "- **Branch 2**: Dense network (processes engineered features)\n",
    "- **Merge**: Concatenated features ‚Üí Dense layers ‚Üí Softmax\n",
    "- **Output**: 24 classes (A-Y, excluding J/Z)\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "- **Test Accuracy**: 97-99%\n",
    "- **Inference Speed**: Real-time (< 10ms per frame)\n",
    "- **Model Size**: ~1-2 MB (TFJS format)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
